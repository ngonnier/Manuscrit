\documentclass[../main]{subfiles}
\ifSubfilesClassLoaded{
    \dominitoc
    \bib
    \tableofcontentsfile
}{}

\begin{document}
\graphicspath{{./figures}}
\chapter{Architectures de cartes auto-organisatrices et nouveaux comportements}
\minitoc



\section{Les cartes auto-organisatrices de Kohonen}\label{sec:som001}

Dans cette thèse, nous nous intéressons aux \emph{cartes auto-organisatrices}, abrégées en SOM (Self-Organizing Maps).
Le modèle de cartes auto-organisatrice a été initialement développé par Kohonen \cite{Kohonen1982}; nous utiliserons ainsi les termes cartes de Kohonen et SOM de façon équivalente pour désigner ce modèle initial.
De nombreux modèles dérivés ont ensuite été développés à partir de ce modèle initial, sur diverses applications.
On décompte par exemple plus de 11000 travaux utilisant les cartes de Kohonen dans la littérature en 2010.
Nous présentons dans cette section le modèle de carte de Kohonen et nous interrogerons sur sa pertinence et les possibilités qu'il offre en tant que module d'une architecture. 

\subsection{Carte de Kohonen classique}

Une carte de Kohonen est un algorithme de quantification vectorielle. C'est à dire, le but est de représenter un ensemble de données d'entrées issues d'un espace $\mathcal{D}$ en un nombre fini de vecteurs de l'espace d'entrée, les prototypes. Dans une SOM, ces prototypes sont disposés sur les noeuds d'un graphe, en général une grille en deux dimensions.
Les noeuds du graphe possèdent alors chacun un prototype et sont \emph{indexés}, en général par un vecteur en deux dimensions lorsque que la carte est une grille.
Cette indexation et le format de graphe permet de définir une distance dans la carte et une notion de voisinage entre noeuds.
Nous appelerons carte de Kohonen le graphe assorti de ses prototypes.

Au début de l'apprentissage, les prototypes prennent une valeur aléatoire dans l'espace d'entrée. 
L'apprentissage est ensuite réalisé en trois étapes:
\begin{enumerate}
\item Une entrée $\inpx$ est présentée à la carte.
\item Le noeud ayant le prototype le plus proche de $\inpx$ selon une distance $d$, généralement la distance euclidienne, est choisie comme \emph{Best Matching Unit} (BMU) de la carte. Son index est notée $\bmu$.
\item Le prototype de la BMU est déplacé vers l'entrée $\inpx$, ainsi que les prototypes des noeuds voisins de $\bmu$ dans un voisinage défini à l'avance. On peut interpréter cette étape comme le déplacement d'une zone de la carte centrée en $\bmu$.
\end{enumerate}

L'algorithme de Kohonen repose donc sur à la fois un processus de compétition, avec la selection de la BMU de la carte, et un processus de coopération avec le déplacement des unités voisines de la BMU.
Toutes les données d'entrées sont tirées dans un même espace $\mathcal{D}$.
Le processus de mise à jour d'une carte de Kohonen se traduit par l'évolution des prototypes de la carte vers des valeurs réparties dans l'espace d'entrée, de façon à ce que n'importe quel vecteur soit proche d'au moins un prototype.
Visuellement, cela correspond à un dépliement de la carte dans l'espace d'entrée. On parlera donc de \emph{dépliement} d'une carte lorsque qu'on parle d'apprentissage. Ce dépliement est représenté en figures \ref{fig:som2d} et \ref{fig:som1d} pour des exemples de cartes en une et deux dimension, se dépliant sur des données en deux dimensions. 
On observe que les valeurs des prototypes à l'issue de l'apprentissage correspondent aux centres de cellules de Voronoï de l'espace d'entrée.
A la fin de l'apprentissage, la carte conserve la structure topologique des entrées:
\begin{itemize}
\item Elle conserve les distances: deux prototypes ayant une distance proche dans la carte seront également proches selon la distance définie dans l'espace d'entrée. On observe donc une contiuité des valeurs des prototypes au sein de la carte.
\item Elle conserve les densités. Une zone de $\mathcal{D}$ présentant plus de vecteurs aura plus d'unités la représentant dans la carte qu'une zone moins dense.
\end{itemize}
La figure \ref{fig:SOM} présente par exemple le dépliement d'une carte sur des imagettes MNIST.
Par son aspect ordonné, une carte est une représentation en faible dimension d'un espace d'entrée de grande dimension. Les cartes de Kohonen sont ainsi utilisées pour visualiser des données de grande dimension et faire du \emph{clustering}. 

% La carte de Kohonen est d'inspiration biologique. Le but premier de Kohonen était de développer un modèle informatique inspiré de l'organisation spatiale des neurones dans le cortex humain, dont un exemple est présenté en figure~\ref{fig:v1}. Il s'est notamment inspiré de l'organisation du cortex en colonnes corticales, ensemble de neurones réagissant à un même stimulus.

% \draft{
% (Kohonen book 1995)
% In an attempt to implement a learning principle that would
% work reliably in practice, effectively creating globally ordered maps of various
% sensory features onto a layered neural network, this author formalized the
% self-organizing process in 1981 and 1982 into an algorithmic form that is
% now being called the Self-Organizing (Feature) Map (SOM) [2.27 -29J. In the
% pure form, the SOM defines an "elastic net" of points (parameter, reference,
% or codebook vectors) that are fitted to the input signal space to approximate
% its density function in an ordered way. The main applications of the SOM
% are thus in the visualization of complex data in a two-dimensional display,
% and creation of abstractions like in many clustering techniques.

% Motivations de Kohonen: capacité des régions du cerveau à s'auto organiser. Note que certes il doit y avoir un prédetermination génétique, mais qu'on observe une réorganisation des mapping lors de déficiences par ex. 

% find abstract self-
% organizing processes in which maps resembling the brain maps are formed,
% whereas it is of less interest whether the maps are formed by evolution, post-
% natal growth, or learning.

% }

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{digits.jpg}
\caption{Représentation de la base de données MNIST, images de chiffres écrits à main levées, par une SOM en deux dimension. Une continuité est observée dans la forme des images lorsqu'on se déplace dans la carte: le $0$ se transforme en $6$, etc.}
\label{fig:SOM}
\end{figure}

\subsection{Aspect topologique de la carte de Kohonen}

La carte de Kohonen se distingue d'autres algorithmes de quantification vectorielle par la topologie introduite par la carte dans l'ensemble des prototypes. Cette topologie dépend du voisinage utilisé par l'algorithme et de la dimension du support de la carte.
La plupart des algorithmes de SOM de la littérature utilisent comme support une grille en deux dimensions. L'indexation des noeuds est alors un ensemble de positions 2D.

En théorie, les cartes peuvent être une dimension (ligne), deux dimensions (grilles), ou de dimension plus grandes. Les cartes peuvent aussi être des graphes de forme plus variable. En pratique, les grilles deux dimension sont les plus couramment utilisées. Elles permettent d'effectuer une réduction de dimension, tout en étant facile à visualiser sur un écran. Les cartes de dimensions supérieures sont très rarement utilisées dans la littérature. Le coût de l'algorithme d'apprentissage dépend en effet du nombre de neurones, et celui-ci augmente exponentiellement lorsqu'on augmente la dimension d'une carte de Kohonen. Les calculs deviennent alors rapidement coûteux.
Les cartes une dimension sont quant à elles limitées en terme de représentation des données, et sont donc rarement utilisées en pratique. Cependant, elles se prêtent mieux à la représentation graphique que les cartes 2D.
De plus, les calculs et l'organisation générés par l'algorithme de Kohonen sont complexes avec des cartes en une dimension. Les travaux conduits en \cite{cottrell_theoretical_2016,fort_soms_2006} apportent une formalisation mathématique de l'algorithme de Kohonen et prouvent la convergence de cartes une dimensions. Les auteurs se heurtent cependant à la preuve de convergence pour des cartes en deux dimensions.
Les processus intervenant dans dans cartes 1D sont donc déjà mathématiquement difficiles à formaliser, difficulté qui augmente fortement avec les dimensions.
L'étude des cartes 1D permet d'envisager des cas simples dans le cadre de développement d'un nouveau modèle de SOM, ce que nous chercherons à faire dans cette thèse, avant de proposer une extension aux cartes 2D.
Les cartes de forme autre que 1D ou 2D sont moins couramment utilisées, mais peuvent avoir des avantages. Ainsi, des cartes structurées en arbre \cite{koikkalainen_self-organizing_1990}, permettant une recherche de BMU structurée. Certains modèles construisent une carte de Kohonen en ajoutant des noeuds au fur et à mesure de l'apprentissage, donnant au final une carte de Kohonen sous forme d'un graphe construit par l'algorithme, tel qu'en \cite{alahakoon_dynamic_2000}.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{soms_topologies}
\caption{Exemples de connexions dans le graphe support d'une SOM. Deux noeuds sont connectés s'il sont à une distance de une unité. Les SOM en deux dimensions sont les plus communément utilisées dans la littérature, sous forme d'une grille ou d'une grille hexagonale. Les SOM une dimension sont également utilisées.}
\label{fig:topo}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{som2d}
\caption{Dépliement d'une SOM 2D sur des données dans le plan $[0,1]^2$, \cite{Kohonen1995SelfOrganizingM} \label{fig:som2d}}

\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{som1d}
\caption{Dépliement d'une SOM 1D sur des données dans un triangle 2D \cite{Kohonen1995SelfOrganizingM}\label{fig:som1d}}

\end{figure}

\subsection{Inspiration Biologique d'une carte de Kohonen}

Le développement des cartes par Kohonen est intiallement inspiré par les cartes topologiques observées dans le cerveau. En effet, si on cartographie la position des neurones par rapport aux stimuli auxquels ils répondent dans certaines zones sensorielles du cerveau, on observe une disposition ordonnée. Les neurones proches réagissent à des stimuli proches. Un exemple est ainsi celui du cortex visuel v1, représenté en figure~\ref{fig:v1}. L'aire associée à l'audition présente aussi une organisation topographique (tonotopic maps), ainsi que de nombreuses autres aires, sensorielles ou plus abstraites \cite{Kohonen1995SelfOrganizingM}.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{v1.jpg}
\caption{Représentation des réponses du cortex visuel V1 à un stimulus visuel (batonnets d'orientation spatiale différentes). Les neurones répondant à une certaine orientation sont affichés de la même couleur. On observe une continuité entre les neurones proches dans le cortex et l'orientation à laquelle ils répondent. Cette propriété d'organisation est l'inspiration biologique des cartes de Kohonen. }
\label{fig:v1}
\end{figure}

\subsection{Les cartes de Kohonen comme modules d'une architecture d'apprentissage complexe ? }

Cette thèse se place dans une démarche de construire un modèle d'architecture permettant un calcul émergent. Nous pensons ainsi que les cartes auto-organisatrices sont de bonnes candidates en tant que modules d'une architecture.
La question d'architecture de cartes de Kohonen a été explorée assez tôt après l'introduction du modèle par Kohonen.
Associer les cartes en modèle hiérarchique faisait même partie des premiers travaux publiés autour des cartes.
Mais étonnament peu de travaux, par rapport à la littérature extrêment étendue sur le sujet des cartes de Kohonen, se sont intéressés à l'association de cartes en tant que nouveau modèle à part entière. Nous passerons en revue les modèles d'architecture existant en section suivante. Avant cela, il est intéressant de s'interroger sur les motivations et les intuitions nous poussant à utiliser une carte de Kohonen en tant que module d'une architecture.

\subsubsection{Inspiration biologique}

Nous avons vu d'un coté que les cartes auto-organisatrices sont d'inspiration biologique; sans chercher à modéliser les neurones du cerveau, leur organisation rappelle des principes généraux rappelant ce qu'on peut observer dans le cerveau. 
Or, on observe dans le cerveau de multiples aires communiquant entre elles. 
Cette communication est observée en biologie lorsque des neurones d'une aire cérébrales activent des neurones d'une autre aire cérébrale. 
Ces connexions à l'échelle des aires cérébrales peuvent être rétroactives, c'est à dire que l'activation entre deux zones se fait dans les deux sens. Par exemple, \cite{primate_cortex_91}: zones dans le cortex du primate. La plupart des connexions est établie dans les deux sens.
La notion d'aire cérébrale renvoie à un aspect modulaire du cerveau. Modules préexistants mais flexibles: ainsi certaines zones qui s'avèrent non utilisées, suite par exemple à la perte d'un sens, se voient réorganisées au profit d'autre zones. On peut donc relier le cerveau à un modèle modulaire, avec des modules apprenant et pouvant se réorganiser.

Ainsi, de la même façon que les cartes auto-organisatrices rappellent l'organisation du cerveau sans chercher à le modéliser, l'architecture biologique observé dans le cerveau justifie l'idée de créer des architectures modulaires de cartes auto-organisatrices.

% On peut explorer plus en détail la notion de modules dans le cerveau. Des zones sont directement liées à des entrées externes, des zones sensorielles, de bas niveau. Le cerveau présente ensuite  d'autres aires liées à ces zones sensorielles, qui apportent de plus en plus d'abstraction dans les représentations des sens. Des aires sont aussi dédiées à l'assocation de plusieurs aires. On parle alors d'architecture hiérarchique, en référence à cette structure de zones sensorielles vers zones abstraites. Cependant, des connexions entre aires peuvent exister dans les deux sens. 

\draft{Le modèle de ces connexions n'est pas établi, mais plusieurs hypothèses et modèles existent en neurosciences computationnelles pour chercher à comprendre ce phénomène. Les plus répandues sont la zone de convergence divergence et la réentrée. A mettre dans modèle computationnels}

\subsubsection{Systèmes autonomes de cartes auto-organisatrices}

Un des enjeux de l'intelligence artificielle est de construire des systèmes autonomes, en robotique par exemple. Les cartes auto-organisatrices ont déjà comme avantage d'être un modèle d'apprentissage non-supervisé: l'ensemble des neurones et des poids réagit aux données présentées pour en dégager des représentations, sans intervention ou retour extérieur. Ensuite s'arrête l'aspect autonome: l'utilisation des cartes pour des tâches applicative nécessite une intervention extérieure. Il faut par exemple étiquetter les poids pour pouvoir faire de la classification d'entrées. La reconstruction d'image utilise la carte comme donnée pour faire du post processing de reconstruction. Utilisée comme algorithme de machine learning, la carte n'est pas un système autonome. 

Kohonen écrivait ainsi dans son livre à propos des enjeux des cartes de Kohonen en 1995:


\begin{quote}Systems of SOMs. A far-reaching goal in self-organization is to create
autonomous systems, the parts of which control each other and learn from
each other. Such control structures may be implemented by special SOMs;
the main problem thereby is the interface, especially automatic scaling of
interconnecting signals between the modules, and picking up relevant signals
to interfaces between modules. We shall leave this idea for future research. \cite{Kohonen1995SelfOrganizingM}
\end{quote}
L'idée d'un système de SOMs s'inscrit dans une recherche de système autonome. En introduisant des connexions entre cartes, on autorise le système à s'auto-activer au lieu de simplement réagir à des entrées, ce qui est nécessaire pour créer un système dynamique. Nous chercherons donc à créer un système de SOMs qui réagit à des entrées, mais qui peut ensuite s'auto-activer.

\subsubsection{Notion de mémoires}

De la même manière que les aires du cerveau, on observe différent types de mémoires interagissant dans un cerveau: mémoire épisodique, mémoire à long terme. Le temps et la mémoire est en quelque sorte spacialisé grâce aux échelles et aux temps de connexions dans le cerveau. 
La notion de mémoire se rapporte finalement aux architectures et systèmes autonomes. Les mêmes éléments de cerveau sont utilisés dans des cadre sensoriels et de mémoire. 
Des systèmes autonomes doivent présenter une mémoire. 
La notion temporelle a donc tout intéret à être intégrée à une architecture de cartes auto-organisatrices dans le cadre de rechercher des systèmes plus autonomes.


\section{Architectures de cartes}

Maintenant que nous avons posé les motivations pour construire une archietcture à partir de cartes auto-organisatrices, nous passons en revue dans cette section les différentes architectures existants dans la littérature. Nous abordons ces modèles d'un point de vue structurel en s'intéressant à comment s'effectue l'interface entre les cartes dans chacun des modèles. 
% Dans cette section, nous passons en revues les modèles existantes permettant, d'une façon ou d'une autre, de connecter des cartes entre elle. Nous étudierons d'un coté les modèles d'architectures de plusieurs cartes communiquant via des interfaces; nous étudierons également les modèles de cartes récurrentes. Les cartes récurrentes sont appliquées à des motifs temporels et ont la caractéristique d'utiliser des éléments de leur état passé pour calculer leur état à un instant donné. La notion de communication est ici encore présente et peut nous servir d'inspiration pour développer une architecture de cartes. Par ailleurs, nous avons mentionné que le traitement de données temporel doit pouvoir être intégrée dans une architecture de carte visant à être autonome. Il s'agit ici de trouver une méthode d'interface entre carte qui puisse à la fois connecter des cartes auto-organisatrices et introduire des connections récurrentes dans le temps. 
Au vu des différents modèles existants, nous avons pu noter deux grandes classes d'architectures de cartes qui sont les architectures \emph{hiérarchiques} et  \emph{non-hiérarchiques.}

\subsection{Architectures hiérarchiques de cartes}

Dès les début du développement des SOMs dans les années 90 par les travaux de Kohonen \cite{Kohonen1982,Kohonen1995SelfOrganizingM}, des travaux proposent des architectures à base de cartes auto-organisatrices. L'enjeu d'associer des cartes auto-organisatrices est également une perspective envisagée par Kohonen dans ses travaux.
Ces architectures sont présentées comme des cartes auto-organisatrices \emph{hiérarchiques} par leurs auteurs. 
Cette hiérarchie désigne une architectures de “SOM de SOM”.
En comparant ces deux travaux, on peut noter que l'architecture HSOM est plus générique que l'arhcitecture proposée en Mikkulaienn, qui est elle spécialement concue pour répondre à la reconnaissance du langage. Nous nous attarderons plus spécifiquement sur ces modèles génériques.

Parmi ces modèles d'architecture, nous pouvons identifier plusieurs mécanismes d'interface entre cartes.
Un grand nombre de travaux s'appuie sur une architecture qu'on peut qualifier de "selective", dont le principe général est illustrée en figure \ref{fig:hsom_selective}.
Dans une architecture hiérachiques, l'activation, le BMU d'une carte d'un niveau au sein de la hiérarchie permet de selectionner une carte d'un niveau suivant. Chacune des cartes du niveau suivant est alors entrainée sur un sous-ensemble des données d'entrées. Ce type d'apprentissage est réalisé niveau par niveau: les cartes d'un même niveau sont entièrement entraînées, puis les cartes du niveau suivant y sont ajoutées et entrainées à leur tour. On ne touche alors plus au premier niveau. 
Ce procédé est retrouvé dans \cite{barbalho_hierarchical_2001,suganthan_pattern_2001,miikkulainen_script_1992,dittenbach_growing_2000,ordonez_hierarchical_2010,zhao_stacked_2015}. L'application des modèles cités est différente, mais le procédé est similaire dans tous ces travaux.
La selection d'une carte pour le niveau suivant repose par exemple sur la position du BMU \cite{barbalho_hierarchical_2001}, une connexion neurone à neurone \cite{??}, ou encore un filtre appliqué sur l'entrée \cite{zhao_stacked_2015}.

\begin{figure}
    \includegraphics[width=\textwidth]{HSOM_selective.pdf}
    \caption{Exemple d'architecture hiérarchique sélective. La carte du premier niveau est entraînée sur tout l'espace d'entrée. Après apprentissage, la carte permet de filtrer les entrées pour les renvoyer vers une carte du niveau suivant. Dans cet exemple, la position du BMU de la carte du niveau 1 permet de sélectionner une carte du niveau 2, comme c'est le cas en \cite{barbalho_hierarchical_2001}. 
    L'entrée permet d'entraîner une carte du deuxième niveau. Chacune des cartes du niveau 2 apprend alors sur un sous-espace d'entrée.\label{fig:hsom_selective}}
\end{figure}

D'un autre coté, un ensemble de travaux repose sur la transmission entre couches de cartes d'information sur la couche précédente. Contrairement aux architectures selectives, la deuxième couche de carte ne prend plus comme entrée un élément de l'espace d'entrée de l'arhcitecture mais travaille sur des éléments des cartes des couches précédentes. L'architecture  HSOM \cite{lampinen_clustering_1992} proposée dès 1990 est composée de deux cartes: une carte apprenant sur des entrées $x$, et un carte prenant comme entrée la position du BMU de la première carte; cette architecture est illustrée en figure~\ref{fig:hsom}.
Comme les cartes s'organisent de façon à conserver les distances dans l'espace d'entrée au sein de la carte, deux éléments faisant partie d'un même groupe de données (cluster) auront des BMUs proches dans la première carte, et leurs BMU dans la seconde carte le seront également. Les auteurs notent ainsi que l'architecture HSOM permet de bien détecter des clusters de données, avec une séparation des clusters un peu meilleure qu'une SOM classique.
Il est intéressant de noter le choix de la position du BMU comme interface entre les cartes ici. Les auteurs de HSOM epxloitent ainsi totalement l'aspect topologique qu'offrent les cartes de Kohonen. D'autres travaux par la suite implémentent des modèles similaires, sur des architectures comportant plus de cartes que HSOM: \cite{hagenauer_hierarchical_2013}
Travaux entre 2000 et 2010:
\cite{dittenbach_growing_2000,yamaguchi_adaptive_2010,gunes_kayacik_hierarchical_2007,wang_comparisonal_2007}

Enfin, on trouve un regain de publications sur les architectures de cartes auto-organisatrices sont publiés après 2015, cette fois sous la terminologie de “Deep SOM”. S'inspirant des réseaux de neurones profonds (deep learning), ayant connu un fort développement dans les années 2010 \cite{lecun_deep_2015}; ces travaux s'intéressent souvent à l'apprentissgage d'image par des SOMs. \cite{Liu2015DeepSM,hankins_somnet_2018,wickramasinghe_deep_2019,aly_deep_2020,sakkari_convolutional_2020,dozono_convolutional_2016,nawaratne_hierarchical_2020,mici_self-organizing_2018} et sont présentés comme des "SOMs convolutionnelles"
Dans ces archis \cite{Liu2015DeepSM,dozono_convolutional_2016}.
Le modèle introduit en \cite{Liu2015DeepSM} est par exemple illustré en figure~\ref{fig:dsom} et est directement inspiré du deep learning. Le but d'une telle architecture est de classifier des images. Une fenêtre est déplacée sur l'image d'entrée, et chaque imagette nourrit alors une carte d'une première couche, donnant $N_{maps}  \times N_{maps}$ positions de BMU $j_{p,q}$. Ces positions représentées comme des valeurs en une dimension sont assemblées en une image intermédiaire, chaque pixel prenant la valeur du BMU de la carte correspondante. Une deuxième étape de fenêtrage peut alors être appliquée sur cette image, et ainsi de suite. La dernière couche du réseau est composée d'une SOM qui effectue alors la tache de classification de l'image intermédiaire, vue comme une représentation abstraite  de l'entrée.
Il est intéressant de souligner l'utilisation des positions des BMUs comme interface entre cartes. L'architecture DSOM s'inscrit ainsi directemnt dans la lignée de HSOM, à la différence que cette fois un vecteur de positions de BMUs est utilisé comme entrée pour la couche suivante.


On note que \cite{Liu2015DeepSM} utilise les positions des BMU comme entrée intermidaire d'une carte, alors que les autres utilisent l'entrée filtrée. Dans ce sens, L'architecture DSOM exploite vraiment l'aspect topologique d'une carte. le auteurs montrent que ce modèle est meilleure qu'une SOM classique dans des taches de classification sur MNIST; les couches supérieures 

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{hsom.pdf}
    \caption{Architecture HSOM \cite{lampinen_clustering_1992}. L'apprentissage des positions du BMU de la première couche par la seconde permet de mieux détecter les ensembles de données, dans une tâche de clustering. La deuxième couche est vue comme un niveau plus abstrait que la première. \label{fig:hsom}}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{DSOM.pdf}
    \caption{Architecture DSOM de SOM "convolutionnelle" \cite{liu_deep_2015}. Les auteurs utilisent les positions des BMUs de la couche de carte $j_{p,q}$ comme valeurs d'entrée pour les couches suivantes, dans la lignée de HSOM \cite{lampinen_clustering_1992}. Les couches sont entraînées les unes après les autres. Cette architecture est \emph{feedforward}, \emph{hiérarchique} mais pas par sélection. \label{fig:dsom}}
\end{figure}

Dans tous ces cas, le but est que la couche finale du réseau possède de meilleurs perf en terme de classification que si on avait utilisé une SOM simple. Les cartes intermédiaires jouent le role de “routeurs” pour selectionner le BMU de la couche finale,  dont les paramètres sont mis à jour au cours de l'apprentissage.
Ces arhcitectures sont “feed forward”: DSOM, … , sont entrainées couches par couche, sans rétro action entre elles. 
Les expériences décrites dans cette biobliographie présentent les arhictetctures comme surpassant les capacité de classification d'une carte simple sur les jeux de données MNIST.
Aly ? evoque des perfs similaire à un réseau de deep learning supervisé
Note : pour une tache de classification, une carte peut eytre comparée à un algo supervisé: la phase de choix de la classe est gérée par un algorithme supervisé après entrainement de la carte.


Ces travaux montrent plusieurs choses:
\begin{itemize}
    \item Il existe différentes façon de transmettre des information entre couches. Les plus courantes sont la transmission de la position du BMU comme entrée pour la carte suivante, ou un caclul d'activation prenant en compte l'activitation de la couche précedente (connexion tous à tous).
    La transmission de la position du BMU , notamment dans l'archi DSOM, montre que ce paradigme permet des bonne capacités de calcul + exploite l'aspect topologique des cartes.
    \item Ces arhcitectyures sont utilisées dans le meme cadre d'application que les SOMs classiques, pour notamment de la classification; les perf surpassent alors l'algo simple. Les travaux HSOM, DSOM notent ainsi une meilleure (plus fine ? ) séparation des classes. La présence de couche multiple a bien généré de nouveaux espaces de calcul et un Meilleur niveau d'abstraction. Ce niveau d'abstraction est pariculirètement utile dans le cadre de la reconnaissance d'images.
\end{itemize}


C'est l'avenement du deep learning qui a poussé à créer des archi de SOMs. De la meme facon que les réseaux profonds ont étendu les capacités d'apprentissage du perceptron, les couhes de SOM montrent la meme ppté.
Est-ce que la recherche actuelle sur l'apprentissage non supervisé poussera à remettre les SOM au gout du jour ? Est-ce qu'on ne fait pas de deep som, simplement parce que les outils n'ont pas été développés comme ceux du deep learning ? L'aspect non linéaire d'une SOM peut pourtant etre prometteuse dans ce cadre d'applis.
Il s'agit par contre d'archiectures completement feed forward: on ne peut pas vraiment parler d'archi modulaires. Les couches sont apprises les unes après les autres.
On fera référence à ce type d'archis comme des archis hiérarchiques.
ces arhcitectures montrent des capacités d'apprentissage de motifs à plus grande echelle qu'une carte simple.

\begin{itemize}
    \item Historique de l'assemblage des SOMs en architectures feedforward ? 
    \item Quels sont les avantages apportés par les deep SOMs par rapport à des Soms classiques
    \item Quels sont les avantages des deep SOM par rapport aux réseaux de deep Learning 
    \item Pourquoi ne sont elles pas plus utilisées maintenant ?
\end{itemize}

\subsection{Strutures de cartes auto-organisatrices non-hiérarchiques}

Face aux architectures hiérarchiques présentées précédemment, nous avons relevé dans la littérature d'autres modèles que nous qualifierons de \emph{non-hiérarchiques}. 


Ces architectures sont en général proposées sous la motivation d'inspiration biologique.
De nombreux travaux de biologie observent en effet des co-activations entre les zones du cerveaux. Le cortex cérébral est ainsi être considéré comme un reseaux de neurone modulaires, avec des régions s'activant entre elles. \cite{primate_cortex_91,mountcastle_columnar_1997,Harriger2012RichCO}
Ces coactivations aient été observées expérimentalement et plusieurs modèles en neurosciences computationnelles ont été proposés pour expliquer ce phénomènes. Les modèles les plus communs sont la zone de convergence-divergence de Damasio (CDZ) \cite{damasio_time-locked_1989}, et le modèle de boucles de réentrées de Edelmann \cite{Edelman1982GroupSA}.
La zone de convergence divergence propose que certains réseaux de neurones dans le cerveau servent de connexions pour associer d'autres zones corticales. Lorsque cette zone est excitée par des signaux en provenance d'une zone corticale, elle propage cette excitation vers les autres zones.
La théorie de la ré-entrée postule quant à elle des connexions directes et réciproques entre  les neurones de différentes cartes cérébrales. Ces connexions permettent la coactivation de neurones dans différentes cartes. 


Dans \cite{dominey13}, les auteurs construisent ainsi des architectures en transmettant les positions des BMU entre les cartes multimodales. L'inspiration est tirée du cadre de la convergence divergence, et les travaux se situent dans un cadre de neuroscience computationnelles.
Chacune des cartes possède plusieurs couches, chacune prenant une modalité en entrée. Une activité est calculée sur ces modalités en une activité commune. La position du BMU, en l'occurence un vecteur 3D, sera utilisée comme modalité hiérachique pour connecter des cartes entre elle. Dans ces travaux, les auteurs présentent une architecture hiérarchique, en s'appuyant sur le modèle CDZ. Une carte amodale sert alors à connecter des cartes modales.
Dans le cas d'une hiérarchie de cartes, les auteurs entrainent les couches de l'architecture séparément: les cartes modales du premier niveau sont apprises, puis la carte amodale les connectant est apprise dans un second temps. 
Une fois toute les cartes apprises, la structure est utilisée pour activer une ou des modalités en activant la carte amodale. Cette carte représente la zone de convergence divergence des modèles cérébraux. 
Notons que ce modèle est d'une certaine façon hiérarchique: les cartes sont entraînées tour à tour et leur connexions apprises dans un second temps. Nous l'avons classée dans non hiérarchique, car une fois que toutes les connexions sont apprises, elle permet des rétroactions entre cartes.

Par leur proximité avec la biologie, les travaux sur les réseaux de neurones impulsionnels ont été plus encleins à développer des modèles de cartes connectées, s'inspirant notamment des deux théories de convergence-divergence et de réentrée.
En \cite{khacef_brain-inspired_2020}, les auteurs passent en revue d'autres modèles de neurosciences computationnelles implémentant de telles architectures au vu de leur plausibilité biologique, et proposent également une architecture de deux cartes auto-organisatrices impulsionnelles pour faire de la fusion de données, en s'inspirant quant à eux de la théorie de la réentrée.
Comme en \cite{dominey13}, les connexions entre cartes sont apprises après apprentissage de chacune. Chaque neurone des deux cartes est connecté aux neurones de l'autre carte par une synapse et un poids synaptique. L'algorithme de mise à jour augmente les poids synaptiques lorsque les neurones aux extrémités de la synapse s'activent en même temps, c'est à dire que les deux neurones sont BMUs en même temps. Après apprentissage, les cartes ont une mémoire associative et autorisent la coactivation entre cartes.

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{MMCM_schema.pdf}
    \caption{Architecture MMCM. Les cartes du premier niveau recoivent l'une les mouvements de tête d'un robot, l'autre les mouvements de main. 
    Une carte amodale (MMCM) recoit les positions des BMUs de chaque carte du premier niveau. Chaque carte du premier niveau possède également une couche "hiérarchique" prenant en entrée les positions du BMU de MCMM. Chaque niveau est entrainé séparément.
    Après apprentissage, l'activation de la carte MCMM produit des mouvements coordonées tête/mains~\cite{dominey13}\label{fig:mmcm}}
\end{figure}


Une autre architecture bio-inspirée est développée en \cite{menard05}. Cette architecture est inspirée de la biologie et repose sur des calcul complètement locaux. Cette archi est également développée dans le contexte multimodal. Des cartes apprenant des modalités sont assemblées en une carte associative, apprenant à associer les activités. Les connexions sont faites par groupes de neurones. Comme l'archi est completement locales, on n'a pas directement la notion de BMU, mais l'idée est tjs de renforcer les connexions et d'autoriser la coactivation de groupes de neurones qui s'activent en meme temps dans différentes cartes.



Les deux modèles mentionnés ci dessus rentrent dans la catégorie non-hiéarchique pour leur possibilité d'activation d'une carte par l'autre. Encore une fois, la position du BMU apparaît chez \cite{dominey13} comme le vecteur de transmission d'information  entre cartes, et suffit pour que la co-activation des cartes permettent de réaliser de la mémoire associative.
Cette mémoire associative est utilisée dans un cadre de données multimodales, avec une notion d'activation d'une carte par l'autre, contrairement aux architectures hiérarchiques citées en section précédentes, utilisées plutot pour des tâches de classification, autrement dit des tâches supervisées.
Dans ces exmples architectures présentées ici, on considère plutot les cartes comme des représentations de leur espace de données et qui permettent de la coactivation entre carte: cela donne une fonction générative à une carte de Kohonen, ce qui n'était pas le cas dans une carte simple.
Cependant, tous ces modèles nécessitent de l'apprentissage hors ligne, avec une mise à jour des cartes en deux étapes: apprentissage des données, puis des relations entre données.


Une autre version d'architecture de cartes non-hiérarchiques est développée en \cite{johnsson_associating_2008}, sous le nom de A-SOM, \emph{associative self-organizing map}. Encore une fois, le but d'une telle architecture est de faire de l'apprentissage multimodal, en apprenant à associer les activités de cartes sur différentes modalités.
La particularité de A-SOM, par rapport à tous les modèles précédemment étudiés, est que l'apprentissage de toutes les cartes et de leurs interaction est réalisé simultanément. 
Cette propriété est intéressante
\begin{figure}
    \centering\includegraphics[width=0.7\textwidth]{A-SOM.pdf}
    \caption{Le modèle A-SOM \cite{johnsson_associating_2008} associe les activités de différentes cartes. Chacune des cartes prend une modalité A,B,C en entrée. Contrairement aux modèles précédemment cites, les trois cartes apprennent simultanément. L'association est prise en compte lors du calcul des activités de chaque carte.\label{fig:asom}}
\end{figure}
\cite{buonamente_hierarchies_2016,Buonamente2013SimulatingAW}: version hiérarchique et version recurrente de asom.



Modèles de \cite{khouzam,menard05}, lefort
A mettre avant ou après les cartes récurrentes, peut etre dans "maintenant qu'on a défini ou on va, on cite les travaux sur lesquels on se base directement et on les place dans la zoologie des cartes ??

\subsubsection{Des modèles multi-cartes incluant des connexions temporelles}

Certains modèles s'appuient sur plusieurs cartes de Kohonen connectées avec une notion de traitement de séquences, comme \cite{parisiLL}. Les auteurs développent une architecture de deux réseaux auto-organisés appelés \emph{grow when required networks}. Ces réseaux sont des versions incrémentales de cartes de Kohonen dans lesquelles des neurones sont ajoutés au cours de l'apprentissage. Le processus de recherche de BMU reste similaire.
cette architecture utilise deux réseaux GWR pour apprendre des séquences, formant une mémoire épisodique et une mémoire sémantique.
La carte associée à la mémoire épisodique (G-EM) est une version récurrente du GWR, dans laquelle des connexions temporelles entre neurones sont mises à jour en supplément des poids associés aux neurones. Le BMU est alors choisi en fonction de l'entrée courante ainsi que des BMUs précédent. 
La deuxième carte est une version classique du GWR. Elle prend en entrée une séquence de BMUs de la carte G-EM, ainsi que la classe de la séquence courante, afin de mettre à jour ses poids. 
Cette architecture associe ainsi des connexions temporelles récurrentes sur une carte ainsi que des connexions entre cartes.
Cette architecture permet des tâches de \emph{lifelong learning}. En général, un algorithme d'apprentissage s'effectue sur un jeu de données d'entrées pré-établi et fini, et est ensuite appliqué sur les tâches pour lesquelles il est concu.
Le concept d'apprentissage sur le long terme s'intéresse à des systèmes étant mis à jour en ligne, dès qu'ils recoivent une entrée, et dont l'apprentissage n'a pas de limite temporelle fixé. On doit donc avoir un système qui trouve de lui-même une stabilité dans l'apprentissage et qui est capable de s'adapter à de nouvelles entrées.
Dans la plupart des applications en robotique, les entrées présentées à une structure d'apprentissage sont par ailleurs des entrées ayant une relation temporelle. Deux images recues successivement par un capteur visuel seront proches dans l'espace des images. Pour une SOM par exemple, cela pose problème. Les arhcitectures de lifelong learning cherchent donc à addresser ces problèmes pour créer une structure autonome, évoluant dans le temps et permettant de réaliser la tâche pour laquelle elle est concue tout en continuant à etre mise à jour, sans oublier les données vues au début de l'apprentissage.
Les auteurs utilisent GDM pour de la reconnaissance d'objets. Cependant, lors de l'apprentissage, les données ne sont pas présentées après un tirage aléatoire dans l'espace, mais sont présentés classe par classe: tous les objets d'une meme classe d'abord, etc. Les auteurs montrent que l'architecture est capable de bien prédire la classe d'un objet lors d'un test sur toutes les classes apprises. A titre de comparaison, une SOM classique apprendrait la classe du premier objet, puis l'oblierait pour se redéplier entièrement sur la deuxième classe, etc. A terme, seule la dernière classe apprise est gardée en mémoire.

La motivation de ce modèle multicarte est intéressante: il s'agit cette fois de voir les deux cartes comme de l'apprentissage à différentes échelles temporelles. L'architecture mélange connexions récurrentes et connexions intercartes, ce qui est pertinent dans le cadre de l'apprenitssage de séquence, et dans le but de création de systèmes autonomes de cartes auto-organisatrices évoquées par Kohonen.


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Parisi_2020.pdf}
    \caption{Architecture \emph{à double mémoire} proposée en \cite{parisiLL}. La couche de mémoire épisodique est un réseau \emph{grow when required (GWR)}, un réseau auto-organisé similaire à une carte de Kohonen, à la différence que les neurones sont ajoutés au fur et à mesure de l'apprentissage. La couche de mémoire sémantique est également un réseau GWR, entraîné à partir des BMUs de la couche épisodique et de la classe de la séquence jouée. L'architecture apprend à reconnaitre plusieurs séquences.\label{fig:gdm_parisi}}
\end{figure}

Cet exemple nous amène à pousser la bibliographie de cette thèse sur les cartes de Kohonen récurrentes. On voit en effet la similarité existant dans le traitement de séquences, ou le choix d'un BMU doit prendre en compte le contexte des états précédents, aux modèles multimodals dans lequel le choix d'un BMU doit prendre en compte le contexte des entrées d'autres modalités.



Travaux de Baheux, ménard, bassem ? 
Ces architectures sont principalement utilisées pour de la génération d'entrée, dans un contexte de mémoire associative. Ou pour de l'apprentissage "lifelong".

\subsection{Cartes auto-organisatrices récurrentes}
\begin{figure}
   \centering\includegraphics[width=0.6\textwidth]{movment_002.pdf}
   \caption{L'image présentée à un réseau (en bleu) correspond à un instant d'une séquence. L'objectif de l'apprentissage non-supervisé de séquences est d'extraire une représentation d'une séquence d'entrée. Une utilisation commune est la classification de mouvements. La séquence "tirer" sera différente de la séquence "marcher".\label{fig:mouvement}}
\end{figure}

On appelle réseaux récurrents des réseaux de neurones qui prennent en compte leur état précédent pour calculer leur état actuel. Ces réseaux sont utilisés pour le traitement et l'apprentissage de signaux temporels. Citons par exemple, en deep learning, les RNN (recurrent neural networks), dont les neurones recoivent leur état précédent en entrée. itons par exemple les LSTM, dans lesquel un système de portes perment d'activer ou non des neurones en fonction des états précédents du réseau. Les cartes de Kohonen ont elles aussi des version récurrentes que nous allons présenter dans cette partie.

Nous nous intéressons aux architectures multi-cartes, mais les cartes récurrentes répondent à des problèmes très similaire à ceux rencontrés dans la conception d'architectures de cartes pour faire de la mémoire associative, comme vu en section précédente.
Dans une carte récurrente, le problème principal est de trouver comment communiquer à la carte de l'information sur son état précédent et comment utiliser cette information dans l'apprentissage de l'état courant. Cela rejoint la problématique posée dans les architectures de cartes de Kohonen, qui est de comment communiquer à une autre carte son état, afin de l'utiliser dans l'apprentissage de l'état courant. L'étude des modèles de cartes récurrentes existant nous permettrons de compléter cette bibliographie. 

Notre volonté de créer un modèle général d'architecture de cartes auto-organisatrice motive également le fait de s'intéresser aux cartes récurrentes. On souhaite en effet créer un modèle qui puisse unir cartes récurrentes et cartes normales au sein d'une même architecture. L'aspect bio-inspiré du modèle et son aspect multimodal ciblent plutot des applications d'un tel réseau en robotique. Or, la plupart des données traitées par des réseaux de neurones, en particulier dans les applications robotiques, sont temporelles : vidéos, signaux sonores, capteurs de position. Il est donc important de s'appuyer autant sur les modèles de cartes récurrentes existantes que sur les architectures afin de créer un modèle général.


Nous avons classé les modèles de cartes récurrentes existants en différentes catégories, qui rejoignent celles observées lors de l'étude des modèles multicartes.
D'une part, certains modèles de cartes utilisent l'état précédent de la carte lors du calcul de \emph{l'activité} de l'état courant. De l'autre, des modèles réutilisent plutot des élements de la carte précedente directement en tant qu'\emph{entrée} de l'état courant.

\subsubsection{The Hypermap architecture, TKM, RSOM}
Parmi les premiers travaux autour des cartes auto-organisatrices, Kohonen propose l'"Hypermap" \cite{Kohonen1991THEHA} L'idée de cette méthode est, pour une carte apprenant sur une séquence d'entrée, de présenter la carte une entrée et un contexte dépendant de l'état précédent. Ce contexte défnit une zone de la carte dans laquelle faire le matching.
\cite{varsta_temporal_2001}
\subsubsection{Recursive SOM, MSOM, SOMSD}
\cite{Voegtlin2002RecursiveSM,hammer_recursive_2004,hammer_self-organizing_2005,hagenbuchner_self-organizing_2003,fix20}
Utilisation comme entrée secondaire à létat suivant l'activité de la carte, de la position du BMU ou d'un mix poids position. 


\section{Axe de recherche choisi}

Nous avons détaillé la littérature existante en terme d'architecture de SOMs et plus généralement de réseaux de neurones auto-organisés. Nous avons divisé ces architectures en deux grandes catégories: un format hiérarchique et \emph{feedforward}, et un format non-hiéarchique incluant des rétroactions.
Le format feedforward implique généralement un apprentissage couche par couche. Ce format est très appliqué et permet d'amélorier les capacités de clustering d'une SOM classique, principalement dans le cas ou les données traitées présentent elles-mêmes une structure hiérarchique, telles que des images ou des phrases.
Nous cherchons à développer une architecture plus générale de cartes auto-organisatrices et ne nous placons ainsi pas dans le contexte des deep SOM mentionné ci-dessus. 
Cependant, nous notons que la position du BMU comme interface entre couche de cartes permet des capacités de calcul.
Nombre de ces architectures sont développées directement dans un but applicatif. On peut ainsi faire la distinction entre un modèle d'architecture, tel que HSOM, qui est générique et applicable à tout type de données, et une structure appliquée, développées spécifiquement pour un type de données.

Opposées à ces arhcitectures hiérarchiques, des architectures reposent sur de l'interaction entre cartes, avec des boucles de rétroaction.
Ces architectures sont moins présentes dans la littérature que les deep SOM, et cherchent en général à se rapprocher d'un contexte biologique, telle que CDZ dominey.
Nous nous placons plutot dans la lignées des cartes non-hiérarchiques, sans vouloir cependant copier un aspect biologique.
De façon intéressante, nous remarquons que plusieurs structures non hiérarchiques sont associées à l'apprentissage de données temporelles. Ces architectures se rapprochent des modèles appélés cartes auto-organisatrices récurrentes, dans lesquels des éléments de calcul d'une carte à une itération données sont réutilisés pour le calcul des itérations suivantes. Ces modèles permettent à une 


\begin{itemize}
    \item Deux grand types d'architectures: hiérachique feedforward et non hiérachique
    \item Utilisation des archis hiérachiques pour améliorer les capacités de classif d'une SOM simple
    \item utilisation des archis non hiérarchiques pour des taches de mémoire associative et de la génération d'activité entre modules
    \item Besoin d'interface entre modules: dans les deux cas, une interface souvent utilisée est la position du BMU. Une autre idée peut etre l'activité des neurones.
    \item Rapprochement des cartes récurrentes avec les architectures non hiérachiques: dans les deux cas, notion de contexte transmis entre carte. Intéret d'associer des cartes récurrente au sein d'archis pour pouvoir par ex faire du traitement de séquence dans un cadre multimodal / lifelong learning
    \item Cartes récurrentes : utilisation de la position du BMU
\end{itemize}

Parmi ces axes de développement d'architectures, nous choisissons de nous intéresser dans cette thèse à des architetcures non-hiérarchiques de cartes.
Les architectures hiérarchiques nous paraissent de bonnes candidates a améliorer les performances d'une SOM sur une application spécifique comme du traitement d'images, tandis que les architectures non-hiéarchiques offrent de nouvelles possibilités de calcul, non envisageables par des SOM classiques, telles que la génération d'entrée, et c'est cet axe que nous souhaitons explorer. L'observation d'émergence de calcul dans des systèmes complexes d'apprentissage, introduite au chapitre~\ref{chap:modularite} vient appuyer ce concept.
% Enfin, les cartes de Kohonen ne se prêtent pas à un apprentissage en ligne de données en temps réel. En effet, il est nécessaires que les données d'entrées soient tirées aléatoirement dans l'espace à apprendre et ne suivent pas une trajectoire. Par exemple, si on veut apprendre des images, mais que les entrées sont données à la carte sous la forme d'une vidéo, les frames de la vidéo sont proches les unes des autres et forment donc une continuité dans l'espace d'entrée des images à apprendre. La carte va alors apprendre le sous ensemble de l'espace sur lequel se trouvent les premières frames, puis les oublier au fur et à mesure de l'apprentissage et les remplacer par les images qui suivent. Avec une architecture multi cartes qui prendrait en compte un aspect temporel, on pourrait imaginer la construction d'une architecture plus robuste.

Dans cette thèse, nous chercherons à développer un modèle d'architecture non-hiérarchique de SOM. Peu de travaux ont exploré l'idée d'associer des SOMs en architectures comportant des rétroactions, et non seulement en architecture hiérarchiques.
Les choix pour la construction d'une telle architecture se situent au niveau de l'interface entre cartes. De nombreux travaux autour des cartes de Kohonen utilisent la position du BMU comme vecteur de l'information transmise entre plusieurs cartes. 
Nous faisons ce choix également. Il apparaît comme une valeur peu coûteuse à communiquer entre cartes, mais qui contient beaucoup d'information sur l'état d'une carte. Cette valeur se présente également comme un cadre homogène de communication intercarte: quelles que soient les entrées sur lesquelles apprend une carte, il sera toujours possible de la connecter à d'autres cartes de l'architecture. Enfin, on retrouve l'utilisation de la position du BMU à la fois dans des architectures multicartes comme HSOM et dans les cartes récurrentes comme SOMSD. Le cadre choisi permettrait donc d'intégrer à la fois des cartes classiques et des cartes récurrentes au sein d'une même architecture, offrant encore plus de possibilités de calcul. Nos travaux se postent dans la lignées des travaux commencés en \cite{baheux_towards_2014} sur des architectures récurrentes multimodales.
%lifelong learning = ouverture ? 
% 

\ifSubfilesClassLoaded{
    \printbibliography
    %\externaldocument{../main.tex}   
}{}
\end{document}