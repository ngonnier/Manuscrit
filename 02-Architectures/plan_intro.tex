\begin{document}
    

\section{Architectures Hiérarchiques}
Modèle  selectifs

Barbalho : se plante sur HSOM. Pas d'équation mais un algo.
Suganthan : supervised SOM. On ajoute une couche tant qu'on est pas satisfait de la qualité. Comment ?


miikkulainen : 
Each unit indep endently determines the input lines with the most variance among its input vectors, and passes a compressed vector down to its submap for a more detailed mapping.
If the data is hierarchical, the map refects this through topological order.
The highest-level map is laid out first in an ordinary self-organizing pro cess. A small map size forces the pro cess to make only a gross, high-level classication. Alower map in the structure receives as its input only those input items which b elong to the category represented by its parent unit. In other words, the unit which \wins" the input item passes this item down to its submap. The lower map forms a sub categorization of these input items, mapping the dierences within the script or the track. Each map displays top ological order. This is most evident in the b ottom level maps, where the PERSON role is dierentiated along one axis, while the other axis separates the bindings of the other op en role of the track. These dimensions were discovered by the mapping itself, and they are dierent for dierent scripts.
he top-level map receives the complete representation vector and maps it onto the unit lab eled REST. This unit compresses the vector by removing the comp onents whose values are the same in all restaurant stories (gure 6). The representation now consists of information which b est distinguishes b etween the dierent restaurant stories.


Zhao : 

Architectures par transmission d'éléments

Ordonez : The proposed architecture uses a tree structure in which each node represents a SOM network. The hierarchical disposition of the nodes indicates different levels of finetuning in the classification (see Figure 2), going from general models to specific models, increasing the detail level as we descend through the hierarchy.
Starting from the representation of a dataset that belongs to a specific domain P = x1,x2,...,xn, where each xi belongs to n, we train a SOM network with the Kohonen algorithm
These weights vectors (models) will serve as an input for the FCM clustering algorithm that will build the so-called metamodels. That information is used to segment the set of input vectors by trying to identify the metamodel that best represents each vector.
Once the segmentation is terminated, a new SOM network is generated for each subset of the inputs, and the training is carried out. On the basis of the set of vectors in P , we generate the sets P1, P2,...Pm, so that P1 intersection P2 is the empty set. FCM technique: This technique consists in a clustering method that makes it possible for an input example to belong to more than one grouping. The model was developed by Dunn in 1973 ([12]) and improved by Bezdek in 1981 ([11]), and is frequently used for pattern recognition. It is based on the minimisation of the following objective function


Modèle HSOM (lampinnen): architecture hiérarchique par transmission d'élément internes

hangenhauer

Equation: 





\end{document}