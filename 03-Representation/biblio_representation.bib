@inproceedings{Belghazi2018MutualIN,
  title={Mutual Information Neural Estimation},
  author={Mohamed Ishmael Belghazi and Aristide Baratin and Sai Rajeswar and Sherjil Ozair and Yoshua Bengio and R. Devon Hjelm and Aaron C. Courville},
  booktitle={ICML},
  year={2018}
}

@article{Bengio2013RepresentationLA,
  title={Representation Learning: A Review and New Perspectives},
  author={Yoshua Bengio and Aaron C. Courville and Pascal Vincent},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2013},
  volume={35},
  pages={1798-1828}
}

@article{Gunning2019XAIExplainableAI,
  title={XAI—Explainable artificial intelligence},
  author={David Gunning and Mark Stefik and Jaesik Choi and Timothy Miller and Simone Stumpf and Guang-Zhong Yang},
  journal={Science Robotics},
  year={2019},
  volume={4}
}

@article{2004kraskov,
   title={Estimating mutual information},
   volume={69},
   ISSN={1550-2376},
   url={http://dx.doi.org/10.1103/PhysRevE.69.066138},
   DOI={10.1103/physreve.69.066138},
   number={6},
   journal={Physical Review E},
   publisher={American Physical Society (APS)},
   author={Kraskov, Alexander and Stögbauer, Harald and Grassberger, Peter},
   year={2004},
   month={Jun}
}

@article{Cottrell1998TheoreticalAO,
  title={Theoretical aspects of the SOM algorithm},
  author={Marie Cottrell and Jean-Claude Fort and Gilles Pag{\`e}s},
  journal={Neurocomputing},
  year={1998},
  volume={21},
  pages={119-138}
}

@inproceedings{Cottrell2016TheoreticalAA,
  title={Theoretical and Applied Aspects of the Self-Organizing Maps},
  author={Marie Cottrell and Madalina Olteanu and Fabrice Rossi and Nathalie Villa-Vialaneix},
  booktitle={WSOM},
  year={2016}
}

@article{Timme2013SynergyRA,
  title={Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective},
  author={Nicholas M. Timme and Wesley Alford and Benjamin Flecker and John M. Beggs},
  journal={Journal of Computational Neuroscience},
  year={2013},
  volume={36},
  pages={119-140}
}

@article{Shannon1948AMT,
  title={A mathematical theory of communication},
  author={Claude E. Shannon},
  journal={Bell Syst. Tech. J.},
  year={1948},
  volume={27},
  pages={379-423}
}

@misc{hjelm2019learning,
      title={Learning deep representations by mutual information estimation and maximization}, 
      author={R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},
      year={2019},
      eprint={1808.06670},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC and Mac Kay, David JC},
  year={2003},
  publisher={Cambridge university press}
}

@article{Theil1961EconomicFA,
  title={Economic Forecasts And Policy Ed. 2nd},
  author={Henri Theil and J. S. Cramer and A. Russchen and H. Moerman},
  journal={Birchandra State Central Library,tripura},
  year={1961}
}

@inproceedings{Doquire2012ACO,
  title={A Comparison of Multivariate Mutual Information Estimators for Feature Selection},
  author={Gauthier Doquire and Michel Verleysen},
  booktitle={ICPRAM},
  year={2012}
}


@article{williams_nonnegative_2010,
	title = {Nonnegative {Decomposition} of {Multivariate} {Information}},
	url = {http://arxiv.org/abs/1004.2515},
	abstract = {Of the various attempts to generalize information theory to multiple variables, the most widely utilized, interaction information, suffers from the problem that it is sometimes negative. Here we reconsider from first principles the general structure of the information that a set of sources provides about a given variable. We begin with a new definition of redundancy as the minimum information that any source provides about each possible outcome of the variable, averaged over all possible outcomes. We then show how this measure of redundancy induces a lattice over sets of sources that clarifies the general structure of multivariate information. Finally, we use this redundancy lattice to propose a definition of partial information atoms that exhaustively decompose the Shannon information in a multivariate system in terms of the redundancy between synergies of subsets of the sources. Unlike interaction information, the atoms of our partial information decomposition are never negative and always support a clear interpretation as informational quantities. Our analysis also demonstrates how the negativity of interaction information can be explained by its confounding of redundancy and synergy.},
	language = {en},
	urldate = {2020-10-27},
	journal = {arXiv:1004.2515 [math-ph, physics:physics, q-bio]},
	author = {Williams, Paul L. and Beer, Randall D.},
	month = apr,
	year = {2010},
	note = {arXiv: 1004.2515},
	keywords = {94A15, Computer Science - Information Theory, Mathematical Physics, Physics - Biological Physics, Physics - Data Analysis, Statistics and Probability, Quantitative Biology - Neurons and Cognition, Quantitative Biology - Quantitative Methods},
	annote = {Comment: 14 pages, 9 figures},
	file = {Williams et Beer - 2010 - Nonnegative Decomposition of Multivariate Informat.pdf:/home/noemie/Zotero/storage/URZP7NXV/Williams et Beer - 2010 - Nonnegative Decomposition of Multivariate Informat.pdf:application/pdf},
}
