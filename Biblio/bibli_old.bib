@article{mici_self-organizing_2018,
	title = {A self-organizing neural network architecture for learning human-object interactions},
	volume = {307},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231218304338},
	doi = {10.1016/j.neucom.2018.04.015},
	abstract = {The visual recognition of transitive actions comprising human-object interactions is a key component for artiﬁcial systems operating in natural environments. This challenging task requires jointly the recognition of articulated body actions as well as the extraction of semantic elements from the scene such as the identity of the manipulated objects. In this paper, we present a self-organizing neural network for the recognition of human-object interactions from RGB-D videos. Our model consists of a hierarchy of Grow-When-Required (GWR) networks that learn prototypical representations of body motion patterns and objects, accounting for the development of action-object mappings in an unsupervised fashion. We report experimental results on a dataset of daily activities collected for the purpose of this study as well as on a publicly available benchmark dataset. In line with neurophysiological studies, our self-organizing architecture exhibits higher neural activation for congruent action-object pairs learned during training sessions with respect to synthetically created incongruent ones. We show that our unsupervised model shows competitive classiﬁcation results on the benchmark dataset with respect to strictly supervised approaches.},
	language = {en},
	urldate = {2018-11-28},
	journal = {Neurocomputing},
	author = {Mici, Luiza and Parisi, German I. and Wermter, Stefan},
	month = sep,
	year = {2018},
	pages = {14--24},
	annote = {Hierarchical process layer by layer
},
	file = {Mici et al. - 2018 - A self-organizing neural network architecture for .pdf:/home/noemie/Zotero/storage/5C7ZBAB8/Mici et al. - 2018 - A self-organizing neural network architecture for .pdf:application/pdf},
}

@article{hagenauer_hierarchical_2013,
	title = {Hierarchical self-organizing maps for clustering spatiotemporal data},
	volume = {27},
	issn = {1365-8816, 1362-3087},
	url = {http://www.tandfonline.com/doi/abs/10.1080/13658816.2013.788249},
	doi = {10.1080/13658816.2013.788249},
	language = {en},
	number = {10},
	urldate = {2019-07-02},
	journal = {International Journal of Geographical Information Science},
	author = {Hagenauer, Julian and Helbich, Marco},
	month = oct,
	year = {2013},
	pages = {2026--2042},
	file = {Hagenauer et Helbich - 2013 - Hierarchical self-organizing maps for clustering s.pdf:/home/noemie/Zotero/storage/6XQ6EFGM/Hagenauer et Helbich - 2013 - Hierarchical self-organizing maps for clustering s.pdf:application/pdf},
}

@inproceedings{cheng_clustering_1992,
	title = {Clustering with competing self-organizing maps},
	volume = {4},
	doi = {10.1109/IJCNN.1992.227222},
	abstract = {Competing self-organizing maps are used to cluster data. Because maps are more complicated than single stereotypes, this clustering is different from k-means clustering in that the proper number of clusters will be discovered. This discovery process for the number of clusters is studied and compared to k-means clustering. Also, because self-organizing maps are probabilistic algorithms, the frequency of a clustering outcome is used as a measure of the validity of the clustering.{\textless}{\textgreater}},
	booktitle = {[{Proceedings} 1992] {IJCNN} {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Cheng, Y.},
	month = jun,
	year = {1992},
	keywords = {Clustering algorithms, Neurons, Neural networks, Convergence, Self organizing feature maps, Computer science, Iterative algorithms, Clustering methods, Frequency measurement, Hebbian theory},
	pages = {785--790 vol.4},
	annote = {Carte de carte : Chaque carte calcule son activite sur une entrée, puis on selectionne la carte qui a la meilleur activité et la mise a jour se fait seuelment dans cette dernière.
Conclusion : quand on a des clusters bien séparés, ils sont mappés séparément.},
	file = {IEEE Xplore Abstract Record:/home/noemie/Zotero/storage/YCICJ9LZ/227222.html:text/html;IEEE Xplore Full Text PDF:/home/noemie/Zotero/storage/VDNT5FA3/Cheng - 1992 - Clustering with competing self-organizing maps.pdf:application/pdf},
}

@inproceedings{wang_comparisonal_2007,
	title = {A comparisonal study of the multi-layer {Kohonen} self-organizing feature maps for spoken language identification},
	doi = {10.1109/ASRU.2007.4430146},
	abstract = {Our previous research indicates that the multi-layer Kohonen self-organizing feature map (MLKSFM) gives a promising performance for spoken language identification (LID). In this paper, we enhance this approach in two distinct ways. Firstly, by considering the phase information, we propose a new type of feature vector which combines the modified group delay function (MODGDF) and the traditional MFCC. Secondly, we propose a hierarchical structure of the MLKSFM, in which the pre-classification is performed at the lower level MLKSFM and the final language identification is performed at the top level MLKSFM. For the OGI-TS speech corpus, the best LID rate is achieved at 87.3\% for the 45-sec test speech utterances by using the hierarchical MLKSFM with 4 classes pre-classified at the lower level MLKSFM. For the 10-sec test speech utterances, the best LID rated is achieved at 60.0\% by using the non-hierarchical MLKSFM LID system.},
	booktitle = {2007 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} {Understanding} ({ASRU})},
	author = {Wang, Liang and Ambikairajah, Eliathamby and Choi, Eric H.C.},
	month = dec,
	year = {2007},
	keywords = {Neural networks, Laboratories, Training data, Australia, Delay, Fourier transforms, hierarchical multi-layer Kohonen self-organizing feature map, Labeling, Language identification, Mel frequency cepstral coefficient, modified group delay function, Natural languages, Speech},
	pages = {402--407},
	annote = {classif sur une carte de K, puis plusieurs cartes apprennent au sein de chaque cluster au niveau sup. Apprentissage en 2 temps.},
	file = {IEEE Xplore Abstract Record:/home/noemie/Zotero/storage/NKKFDQU9/4430146.html:text/html;IEEE Xplore Full Text PDF:/home/noemie/Zotero/storage/5QXGDFA6/Wang et al. - 2007 - A comparisonal study of the multi-layer Kohonen se.pdf:application/pdf},
}

@inproceedings{liu_deep_2015,
	title = {Deep {Self}-{Organizing} {Map} for visual classification},
	doi = {10.1109/IJCNN.2015.7280357},
	abstract = {We proposed a Deep Self-Organizing Map (DSOM) algorithm which is completely different from the existing multi-layers SOM algorithms, such as SOINN. It consists of layers of alternating self-organizing map and sampling operator. The self-organizing layer is made up of certain numbers of SOMs, with each map only looking at a local region block on its input. The winning neuron's index value from every SOM in self-organizing layer is then organized in the sampling layer to generate another 2D map, which could then be fed to a second self-organizing layer. In this way, local information is gathered together, forming more global information in higher layers. The construction method of the DSOM is unique and will be introduced in this paper. Experiments were carried out to discuss how the DSOM architecture parameters affect the performance. We evaluate our proposed DSOM on MNIST and CASIA-HWDB1.1 dataset. Experimental results show that DSOM outperforms the original supervised SOM by 7:17\% on MNIST and 7:25\% on CASIA-HWDB1.1.},
	booktitle = {2015 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Liu, Nan and Wang, Jinjun and Gong, Yihong},
	month = jul,
	year = {2015},
	note = {ISSN: 2161-4407},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/noemie/Zotero/storage/VXG9EZ6T/7280357.html:text/html;IEEE Xplore Full Text PDF:/home/noemie/Zotero/storage/57DBHYWQ/Liu et al. - 2015 - Deep Self-Organizing Map for visual classification.pdf:application/pdf},
}

@article{lampinen_clustering_1992,
	title = {Clustering {Properties} of {Hierarchical} {Self}-{Organizing} {Maps}},
	volume = {2},
	abstract = {A multilayer self-organizing map, HSOM, is discussed as an unsupervised clustering method. The HSOM is shown to form arbitrarily complex clusters, in analogy with multilayer feedforward networks. In addition, the HSOM provides a natural measure for the distance of a point from a cluster that weighs all the points belonging to the cluster appropriately.},
	language = {en},
	journal = {Journal of Mathematical Imaging and Vision},
	author = {Lampinen, Jouko and Oja, Erkki},
	year = {1992},
	file = {Lampinen and Oja - Clustering Properties of Hierarchical Self-Organiz.pdf:/home/noemie/Zotero/storage/VFQHX86D/Lampinen and Oja - Clustering Properties of Hierarchical Self-Organiz.pdf:application/pdf},
}

@article{gunes_kayacik_hierarchical_2007,
	title = {A hierarchical {SOM}-based intrusion detection system},
	volume = {20},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197606001606},
	doi = {10.1016/j.engappai.2006.09.005},
	abstract = {Purely based on a hierarchy of self-organizing feature maps (SOMs), an approach to network intrusion detection is investigated. Our principle interest is to establish just how far such an approach can be taken in practice. To do so, the KDD benchmark data set from the International Knowledge Discovery and Data Mining Tools Competition is employed. Extensive analysis is conducted in order to assess the signiﬁcance of the features employed, the partitioning of training data and the complexity of the architecture. Contributions that follow from such a holistic evaluation of the SOM include recognizing that (1) best performance is achieved using a two-layer SOM hierarchy, based on all 41-features from the KDD data set. (2) Only 40\% of the original training data is sufﬁcient for training purposes. (3) The ‘Protocol’ feature provides the basis for a switching parameter, thus supporting modular solutions to the detection problem. The ensuing detector provides false positive and detection rates of 1.38\% and 90.4\% under test conditions; where this represents the best performance to date of a detector based on an unsupervised learning algorithm.},
	language = {en},
	number = {4},
	urldate = {2021-07-09},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Gunes Kayacik, H. and Nur Zincir-Heywood, A. and Heywood, Malcolm I.},
	month = jun,
	year = {2007},
	pages = {439--451},
	file = {Gunes Kayacik et al. - 2007 - A hierarchical SOM-based intrusion detection syste.pdf:/home/noemie/Zotero/storage/3PZAMBLY/Gunes Kayacik et al. - 2007 - A hierarchical SOM-based intrusion detection syste.pdf:application/pdf},
}

@inproceedings{barbalho_hierarchical_2001,
	title = {Hierarchical {SOM} applied to image compression},
	volume = {1},
	doi = {10.1109/IJCNN.2001.939060},
	abstract = {The increase of the need for image storage and transmission in computer systems has increased the importance of signal and image compression algorithms. The approach involving vector quantization (VQ) relies on the design of a finite set of codes which will substitute the original signal during transmission with a minimal of distortion, taking advantage of the spatial redundancy of image to compress them. Algorithms such as LBG and SOM work in an unsupervised way toward finding a good codebook for a given training data. However, the number of code vectors (N) needed for VQ increases with the vector dimension, and full-search algorithms such as LBG and SOM can lead to large training and coding times. An alternative for reducing the computational complexity is the use of a tree-structured vector quantization algorithm. This paper presents an application of a hierarchical SOM for image compression which reduces the search complexity from O(N) to O(log N), enabling a faster training and image coding. Results are given for conventional SOM, LBG and HSOM, showing the advantage of the proposed method.},
	booktitle = {{IJCNN}'01. {International} {Joint} {Conference} on {Neural} {Networks}. {Proceedings} ({Cat}. {No}.{01CH37222})},
	author = {Barbalho, J.M. and Duarte, A. and Neto, D. and Costa, J.A.F. and Netto, M.L.A.},
	month = jul,
	year = {2001},
	note = {ISSN: 1098-7576},
	keywords = {Vector quantization, Image coding, Training data, Image reconstruction, Computational complexity, Costs, Distortion, Image storage, Signal design, Storage automation},
	pages = {442--447 vol.1},
	file = {IEEE Xplore Abstract Record:/home/noemie/Zotero/storage/FVIYN756/939060.html:text/html;IEEE Xplore Full Text PDF:/home/noemie/Zotero/storage/3K6NDTJB/Barbalho et al. - 2001 - Hierarchical SOM applied to image compression.pdf:application/pdf},
}

@article{furao_incremental_2006,
	title = {An incremental network for on-line unsupervised classification and topology learning},
	volume = {19},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608005000845},
	doi = {10.1016/j.neunet.2005.04.006},
	abstract = {This paper presents an on-line unsupervised learning mechanism for unlabeled data that are polluted by noise. Using a similarity thresholdbased and a local error-based insertion criterion, the system is able to grow incrementally and to accommodate input patterns of on-line nonstationary data distribution. A deﬁnition of a utility parameter, the error-radius, allows this system to learn the number of nodes needed to solve a task. The use of a new technique for removing nodes in low probability density regions can separate clusters with low-density overlaps and dynamically eliminate noise in the input data. The design of two-layer neural network enables this system to represent the topological structure of unsupervised on-line data, report the reasonable number of clusters, and give typical prototype patterns of every cluster without prior conditions such as a suitable number of nodes or a good initial codebook.},
	language = {en},
	number = {1},
	urldate = {2021-07-12},
	journal = {Neural Networks},
	author = {Furao, Shen and Hasegawa, Osamu},
	month = jan,
	year = {2006},
	pages = {90--106},
	file = {Furao and Hasegawa - 2006 - An incremental network for on-line unsupervised cl.pdf:/home/noemie/Zotero/storage/F9RPUL72/Furao and Hasegawa - 2006 - An incremental network for on-line unsupervised cl.pdf:application/pdf},
}

@inproceedings{ritter_combining_1989,
	address = {Washington, DC, USA},
	title = {Combining self-organizing maps},
	url = {http://ieeexplore.ieee.org/document/118289/},
	doi = {10.1109/IJCNN.1989.118289},
	abstract = {Kohonen’s self-organizing maps provide a model for a neural module capable of learning interesting input-output-relationships by forming an adaptive table. In this paper we suggest a way to combine several such modules in order to learn more complex tasks. Our approach is applicable whenever the function to be learnt can be decomposed in a known way into a number of unknown functions, for each of which a separate module is assigned.},
	language = {en},
	urldate = {2021-09-23},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {{Ritter}},
	year = {1989},
	pages = {499--502 vol.2},
	file = {Ritter - 1989 - Combining self-organizing maps.pdf:/home/noemie/Zotero/storage/WYEPQCK5/Ritter - 1989 - Combining self-organizing maps.pdf:application/pdf},
}

@article{ordonez_hierarchical_2010,
	title = {Hierarchical {Clustering} {Analysis} with {SOM} {Networks}},
	volume = {4},
	url = {https://publications.waset.org/6849/hierarchical-clustering-analysis-with-som-networks},
	abstract = {Hierarchical Clustering Analysis with SOM Networks},
	language = {en},
	number = {9},
	urldate = {2022-01-11},
	journal = {International Journal of Computer and Information Engineering},
	author = {Ordonez, Diego and Dafonte, Carlos and Manteiga, Minia and Arcayy, Bernardino},
	month = sep,
	year = {2010},
	pages = {1393--1399},
	file = {Full Text PDF:/home/noemie/Zotero/storage/VFR45S9D/Ordonez et al. - 2010 - Hierarchical Clustering Analysis with SOM Networks.pdf:application/pdf;Snapshot:/home/noemie/Zotero/storage/2S7MNJIE/hierarchical-clustering-analysis-with-som-networks.html:text/html},
}

@inproceedings{yamaguchi_adaptive_2010,
	title = {Adaptive {Learning} {Algorithm} in {Tree}-{Structured} {Self}-{Organizing} {Feature} {Map}},
	abstract = {Self-Organizing Feature Map is a layered neural network consisting of an input layer and a competitive layer for the data visualization and vector quantization. The accuracy of SOM vector quantization depends on the number of competitive layer's neurons. Therefore, when an unknown data set is given, it is difficult to decide the sufficient competitive layer size. In this paper, we propose a hierarchical competitive layer adaptation method in order to find out the sufficient number of neurons. The proposed method adds and deletes neurons using the means error and frequency in use among neighboring neurons.},
	language = {en},
	booktitle = {{SCIS}\&{ISIS}, {Okayoma}},
	author = {Yamaguchi, Takashi and Ichimura, Takumi and Mackin, Kenneth James},
	month = dec,
	year = {2010},
	pages = {6},
	file = {Yamaguchi et al. - Adaptive Learning Algorithm in Tree-Structured Sel.pdf:/home/noemie/Zotero/storage/3UFER3HD/Yamaguchi et al. - Adaptive Learning Algorithm in Tree-Structured Sel.pdf:application/pdf},
}

@inproceedings{dittenbach_growing_2000,
	address = {Como, Italy},
	title = {The growing hierarchical self-organizing map},
	isbn = {978-0-7695-0619-7},
	url = {http://ieeexplore.ieee.org/document/859366/},
	doi = {10.1109/IJCNN.2000.859366},
	abstract = {In this paper we present the growing hierarchical self-organizing map. This dynamically growing neural network model evolves into a hierarchical structure according to the requirements of the input data during an unsupervised training process. We demonstrate the beneﬁts of this novel neural network model by organizing a real-world document collection according to their similarities.},
	language = {en},
	urldate = {2022-01-12},
	booktitle = {Proceedings of the {IEEE}-{INNS}-{ENNS} {International} {Joint} {Conference} on {Neural} {Networks}. {IJCNN} 2000. {Neural} {Computing}: {New} {Challenges} and {Perspectives} for the {New} {Millennium}},
	publisher = {IEEE},
	author = {Dittenbach, M. and Merkl, D. and Rauber, A.},
	year = {2000},
	pages = {15--19 vol.6},
	file = {Dittenbach et al. - 2000 - The growing hierarchical self-organizing map.pdf:/home/noemie/Zotero/storage/K5YTRN3R/Dittenbach et al. - 2000 - The growing hierarchical self-organizing map.pdf:application/pdf},
}

@article{jung_self-organization_2015,
	title = {Self-{Organization} of {Spatio}-{Temporal} {Hierarchy} via {Learning} of {Dynamic} {Visual} {Image} {Patterns} on {Action} {Sequences}},
	volume = {10},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0131214},
	doi = {10.1371/journal.pone.0131214},
	language = {en},
	number = {7},
	urldate = {2022-01-12},
	journal = {PLOS ONE},
	author = {Jung, Minju and Hwang, Jungsik and Tani, Jun},
	editor = {Weng, Xuchu},
	month = jul,
	year = {2015},
	pages = {e0131214},
	file = {Jung et al. - 2015 - Self-Organization of Spatio-Temporal Hierarchy via.pdf:/home/noemie/Zotero/storage/IFMFW7CL/Jung et al. - 2015 - Self-Organization of Spatio-Temporal Hierarchy via.pdf:application/pdf},
}

@article{nawaratne_hierarchical_2020,
	title = {Hierarchical {Two}-{Stream} {Growing} {Self}-{Organizing} {Maps} {With} {Transience} for {Human} {Activity} {Recognition}},
	volume = {16},
	issn = {1941-0050},
	doi = {10.1109/TII.2019.2957454},
	abstract = {The rapid growth in autonomous industrial environments has increased the need for intelligent video surveillance. As a predominant element of video surveillance, recognition of complex human movements is important in a wide range of surveillance applications. However, the current state-of-the-art video surveillance techniques use supervised deep learning pipelines for human activity recognition (HAR). A key shortcoming of such techniques is the inability to learn from unlabeled video streams. To operate effectively in natural environments, video surveillance techniques have to be able to handle huge volumes of unlabeled video data, monitor and generate alerts and insights derived from multiple characteristics such as spatial structure, motion flow, color distribution, etc. Furthermore, most conventional learning systems lack memory persistence capability which can reduce the influence of outdated information in memory-guided decision-making resulting in limiting plasticity and overfitting based on specific past events. In this article, we propose a new adaptation of the Growing Self-Organizing Map (GSOM) to address these shortcomings by 1) adopting two proven concepts of traditional deep learning, hierarchical, and multistream learning, applied into GSOM self-structuring architecture to accommodate learning from unlabeled video data and their diverse characteristics, 2) address overfitting and the influence of outdated information on neural architecture by implementing a transience property in the algorithm. We demonstrate the proposed model using three benchmark video datasets and the results confirm its validity and usability for HAR.},
	number = {12},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Nawaratne, Rashmika and Alahakoon, Damminda and De Silva, Daswin and Kumara, Harsha and Yu, Xinghuo},
	month = dec,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	keywords = {Neurons, self-organizing maps, Histograms, neural networks, Self-organizing feature maps, Feature extraction, Activity recognition, Forgetting, hierarchical learning, human activity recognition (HAR), Optical flow, Video surveillance},
	pages = {7756--7764},
	file = {IEEE Xplore Abstract Record:/home/noemie/Zotero/storage/F2AYCU6R/8922590.html:text/html;IEEE Xplore Full Text PDF:/home/noemie/Zotero/storage/9X5W7ZDU/Nawaratne et al. - 2020 - Hierarchical Two-Stream Growing Self-Organizing Ma.pdf:application/pdf},
}

@inproceedings{ferles_deep_2021,
	title = {Deep {Learning} {Self}-{Organizing} {Map} of {Convolutional} {Layers}},
	url = {https://aircconline.com/csit/papers/vol11/csit110303.pdf},
	doi = {10.5121/csit.2021.110303},
	abstract = {The Self-Organizing Convolutional Map (SOCOM) combines convolutional neural networks, clustering via self-organizing maps, and learning through gradient backpropagation into a novel unified unsupervised deep architecture. The proposed clustering and training procedures reflect the model’s degree of integration and synergy between its constituting modules. The SOCOM prototype is in position to carry out unsupervised classification and clustering tasks based upon the distributed higher level representations that are produced by its underlying convolutional deep architecture, without necessitating target or label information at any stage of its training and inference operations. Due to its convolutional component SOCOM has the intrinsic capability to model signals consisting of one or more channels like grayscale and colored images.},
	language = {en},
	urldate = {2022-07-14},
	booktitle = {Computer {Science} \& {Information} {Technology} ({CS} \& {IT})},
	publisher = {AIRCC Publishing Corporation},
	author = {Ferles, Christos and Papanikolaou, Yannis and P. Savaidis, Stylianos and A. Mitilineos, Stelios},
	month = mar,
	year = {2021},
	pages = {25--32},
	file = {Ferles et al. - 2021 - Deep Learning Self-Organizing Map of Convolutional.pdf:/home/noemie/Zotero/storage/5IK2KI3V/Ferles et al. - 2021 - Deep Learning Self-Organizing Map of Convolutional.pdf:application/pdf},
}

@article{aly_deep_2020,
	title = {Deep {Convolutional} {Self}-{Organizing} {Map} {Network} for {Robust} {Handwritten} {Digit} {Recognition}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9110900/},
	doi = {10.1109/ACCESS.2020.3000829},
	abstract = {Deep Convolutional Neural Networks (DCNN) are currently the predominant technique commonly used to learn visual features from images. However, the complex structure of most recent DCNNs impose two major requirements namely, huge labeled dataset and high computational resources. In this paper, we develop a new efﬁcient deep unsupervised network to learn invariant image representation from unlabeled visual data. The proposed Deep Convolutional Self-organizing Maps (DCSOM) network comprises a cascade of convolutional SOM layers trained sequentially to represent multiple levels of features. The 2D SOM grid is commonly used for either data visualization or feature extraction. However, this work employs high dimensional map size to create a new deep network. The N-Dimensional SOM (ND-SOM) grid is trained to extract abstract visual features using its classical competitive learning algorithm. The topological order of the features learned from ND-SOM helps to absorb local transformation and deformation variations exhibited in the visual data. The input image is divided into an overlapped local patches where each local patch is represented by the N-coordinates of the winner neuron in the ND-SOM grid. Each dimension of the ND-SOM can be considered as a non-linear principal component and hence it can be exploited to represent the input image using N-Feature Index Image (FII) bank. Multiple convolutional SOM layers can be cascaded to create a deep network structure. The output layer of the DCSOM network computes local histograms of each FII bank in the ﬁnal convolutional SOM layer. A set of experiments using MNIST handwritten digit database and all its variants are conducted to evaluate the robust representation of the proposed DCSOM network. Experimental results reveal that the performance of DCSOM outperforms state-of-the-art methods for noisy digits and achieve a comparable performance with other complex deep learning architecture for other image variations.},
	language = {en},
	urldate = {2022-07-14},
	journal = {IEEE Access},
	author = {Aly, Saleh and Almotairi, Sultan},
	year = {2020},
	pages = {107035--107045},
	annote = {SOm pour le traitement d’image. 
Division d’une image en patches, qui seront ensuite combinés par une nouvelle SOM. 
},
	file = {Aly and Almotairi - 2020 - Deep Convolutional Self-Organizing Map Network for.pdf:/home/noemie/Zotero/storage/VL7XI3MA/Aly and Almotairi - 2020 - Deep Convolutional Self-Organizing Map Network for.pdf:application/pdf},
}

@inproceedings{hankins_somnet_2018,
	address = {Rio de Janeiro},
	title = {{SOMNet}: {Unsupervised} {Feature} {Learning} {Networks} for {Image} {Classification}},
	isbn = {978-1-5090-6014-6},
	shorttitle = {{SOMNet}},
	url = {https://ieeexplore.ieee.org/document/8489404/},
	doi = {10.1109/IJCNN.2018.8489404},
	abstract = {We present here an unsupervised approach to learning suitable features for a deep learning framework applied to image classiﬁcation. PCANet was introduced as a simple and efﬁcient baseline for deep learning approaches which used cascaded principle component analysis (PCA) derived ﬁlter banks, as well as other simple image processing elements such as binary hashing and blockwise histograms. This was followed by DCTNet which used discrete cosine transform (DCT) ﬁlter banks as a learning-free alternative. In this paper we propose SOMNet which uses self-organizing map (SOM) based ﬁlters offering a non-orthogonal alternative to PCANet providing comparable performance. It is well established that SOM is a non-linear version of PCA but does not suffer from the same constraints. We also show that through the use of a simple trick in the binarization process results in a dramatic reduction in the dimension of the ﬁnal feature vector, thus allowing the utilization of more ﬁlters which could lead to deeper and more complex structures in further work. We also demonstrate the results of a hybrid methodology that clusters generative Markov random ﬁelds (MRF) as ﬁlters which provides more diverse features in a data driven approach to deep learning.},
	language = {en},
	urldate = {2022-07-18},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Hankins, Richard and Peng, Yao and Yin, Hujun},
	month = jul,
	year = {2018},
	pages = {1--8},
	file = {Hankins et al. - 2018 - SOMNet Unsupervised Feature Learning Networks for.pdf:/home/noemie/Zotero/storage/5878XRNX/Hankins et al. - 2018 - SOMNet Unsupervised Feature Learning Networks for.pdf:application/pdf},
}

@article{chan_pcanet_2015,
	title = {{PCANet}: {A} {Simple} {Deep} {Learning} {Baseline} for {Image} {Classification}?},
	volume = {24},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{PCANet}},
	url = {http://arxiv.org/abs/1404.3606},
	doi = {10.1109/TIP.2015.2475625},
	abstract = {In this work, we propose a very simple deep learning network for image classiﬁcation which comprises only the very basic data processing components: cascaded principal component analysis (PCA), binary hashing, and block-wise histograms. In the proposed architecture, PCA is employed to learn multistage ﬁlter banks. It is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus named as a PCA network (PCANet) and can be designed and learned extremely easily and efﬁciently. For comparison and better understanding, we also introduce and study two simple variations to the PCANet, namely the RandNet and LDANet. They share the same topology of PCANet but their cascaded ﬁlters are either selected randomly or learned from LDA. We have tested these basic networks extensively on many benchmark visual datasets for different tasks, such as LFW for face veriﬁcation, MultiPIE, Extended Yale B, AR, FERET datasets for face recognition, as well as MNIST for hand-written digits recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state of the art features, either preﬁxed, highly hand-crafted or carefully learned (by DNNs). Even more surprisingly, it sets new records for many classiﬁcation tasks in Extended Yale B, AR, FERET datasets, and MNIST variations. Additional experiments on other public datasets also demonstrate the potential of the PCANet serving as a simple but highly competitive baseline for texture classiﬁcation and object recognition.},
	language = {en},
	number = {12},
	urldate = {2022-07-18},
	journal = {IEEE Transactions on Image Processing},
	author = {Chan, Tsung-Han and Jia, Kui and Gao, Shenghua and Lu, Jiwen and Zeng, Zinan and Ma, Yi},
	month = dec,
	year = {2015},
	note = {arXiv:1404.3606 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	pages = {5017--5032},
	file = {Chan et al. - 2015 - PCANet A Simple Deep Learning Baseline for Image .pdf:/home/noemie/Zotero/storage/ZHXH6BCA/Chan et al. - 2015 - PCANet A Simple Deep Learning Baseline for Image .pdf:application/pdf},
}

@article{lawrence_face_1997,
	title = {Face recognition: a convolutional neural-network approach},
	volume = {8},
	issn = {1941-0093},
	shorttitle = {Face recognition},
	doi = {10.1109/72.554195},
	abstract = {We present a hybrid neural-network for human face recognition which compares favourably with other methods. The system combines local image sampling, a self-organizing map (SOM) neural network, and a convolutional neural network. The SOM provides a quantization of the image samples into a topological space where inputs that are nearby in the original space are also nearby in the output space, thereby providing dimensionality reduction and invariance to minor changes in the image sample, and the convolutional neural network provides partial invariance to translation, rotation, scale, and deformation. The convolutional network extracts successively larger features in a hierarchical set of layers. We present results using the Karhunen-Loeve transform in place of the SOM, and a multilayer perceptron (MLP) in place of the convolutional network for comparison. We use a database of 400 images of 40 individuals which contains quite a high degree of variability in expression, pose, and facial details. We analyze the computational complexity and discuss how new classes could be added to the trained recognizer.},
	number = {1},
	journal = {IEEE Transactions on Neural Networks},
	author = {Lawrence, S. and Giles, C.L. and Tsoi, Ah Chung and Back, A.D.},
	month = jan,
	year = {1997},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Humans, Neural networks, Spatial databases, Feature extraction, Face recognition, Image databases, Image sampling, Karhunen-Loeve transforms, Multilayer perceptrons, Quantization},
	pages = {98--113},
	file = {IEEE Xplore Abstract Record:/home/noemie/Zotero/storage/955LLLBA/554195.html:text/html;IEEE Xplore Full Text PDF:/home/noemie/Zotero/storage/CSHS7LTJ/Lawrence et al. - 1997 - Face recognition a convolutional neural-network a.pdf:application/pdf},
}

@article{mohebi_convolutional_2014,
	title = {A convolutional recursive modified {Self} {Organizing} {Map} for handwritten digits recognition},
	volume = {60},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608014001968},
	doi = {10.1016/j.neunet.2014.08.001},
	abstract = {It is well known that the handwritten digits recognition is a challenging problem. Different classification algorithms have been applied to solve it. Among them, the Self Organizing Maps (SOM) produced promising results. In this paper, first we introduce a Modified SOM for the vector quantization problem with improved initialization process and topology preservation. Then we develop a Convolutional Recursive Modified SOM and apply it to the problem of handwritten digits recognition. The computational results obtained using the well known MNIST dataset demonstrate the superiority of the proposed algorithm over the existing SOM-based algorithms.},
	language = {en},
	urldate = {2022-07-18},
	journal = {Neural Networks},
	author = {Mohebi, Ehsan and Bagirov, Adil},
	month = dec,
	year = {2014},
	pages = {104--118},
	file = {Mohebi and Bagirov - 2014 - A convolutional recursive modified Self Organizing.pdf:/home/noemie/Zotero/storage/456IW46T/Mohebi and Bagirov - 2014 - A convolutional recursive modified Self Organizing.pdf:application/pdf},
}

@inproceedings{dozono_convolutional_2016,
	title = {Convolutional {Self} {Organizing} {Map}},
	doi = {10.1109/CSCI.2016.0149},
	abstract = {Recently, deep learning became very popular, and was applied to many fields. The convolutional neural networks are often used for representing the layers for deep learning. In this paper, we propose Convolutional Self Organizing Map, which can be applicable to deep learning. Conventional Self Organizing Map uses single layered architecture, and can visualizes and classifies the input data on 2 dimensional map. SOMs which uses multiple layers are already proposed. In this paper, we propose Self Organizing Map algorithms which include convolutional layers. 2 types of convolution methods, which are based on conventional method and inspired from Self Organizing Map algorithm are proposed, and the performance of both method is examined in the experiments of clustering image data.},
	booktitle = {2016 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Dozono, Hiroshi and Niina, Gen and Araki, Satoru},
	month = dec,
	year = {2016},
	keywords = {Correlation, Data visualization, Neural networks, Arrays, Convolution, Convolutional neural network, Deep Learning, Kernel, Machine learning, Self Organizing Map},
	pages = {767--771},
	file = {IEEE Xplore Abstract Record:/home/noemie/Zotero/storage/5XJU2E2S/7881442.html:text/html;IEEE Xplore Full Text PDF:/home/noemie/Zotero/storage/FTH2BFWC/Dozono et al. - 2016 - Convolutional Self Organizing Map.pdf:application/pdf},
}

@article{ferles_denoising_2018,
	title = {Denoising {Autoencoder} {Self}-{Organizing} {Map} ({DASOM})},
	volume = {105},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608018301382},
	doi = {10.1016/j.neunet.2018.04.016},
	abstract = {In this report, we address the question of combining nonlinearities of neurons into networks for modeling increasingly varying and progressively more complex functions. A fundamental approach is the use of higher-level representations devised by restricted Boltzmann machines and (denoising) autoencoders. We present the Denoising Autoencoder Self-Organizing Map (DASOM) that integrates the latter into a hierarchically organized hybrid model where the front-end component is a grid of topologically ordered neurons. The approach is to interpose a layer of hidden representations between the input space and the neural lattice of the self-organizing map. In so doing the parameters are adjusted by the proposed unsupervised learning algorithm. The model therefore maintains the clustering properties of its predecessor, whereas by extending and enhancing its visualization capacity enables an inclusion and an analysis of the intermediate representation space. A comprehensive series of experiments comprising optical recognition of text and images, and cancer type clustering and categorization is used to demonstrate DASOM’s efficiency, performance and projection capabilities.},
	language = {en},
	urldate = {2022-07-18},
	journal = {Neural Networks},
	author = {Ferles, Christos and Papanikolaou, Yannis and Naidoo, Kevin J.},
	month = sep,
	year = {2018},
	pages = {112--131},
	file = {Ferles et al. - 2018 - Denoising Autoencoder Self-Organizing Map (DASOM).pdf:/home/noemie/Zotero/storage/SC4HSCEL/Ferles et al. - 2018 - Denoising Autoencoder Self-Organizing Map (DASOM).pdf:application/pdf},
}

@article{sakkari_convolutional_2020,
	title = {A {Convolutional} {Deep} {Self}-{Organizing} {Map} {Feature} extraction for machine learning},
	volume = {79},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-020-08822-9},
	doi = {10.1007/s11042-020-08822-9},
	abstract = {In this work we propose a new Unsupervised Deep Self-Organizing Map (UDSOM) algorithm for feature extraction, quite similar to the existing multi-layer SOM architectures. The principal underlying idea of using SOMs is that if a neuron is wins n times, these n inputs that activated this neuron are similar. The basic principle consists of an alternation of phases of splitting and abstraction of regions, based on a non-linear projection of high-dimensional data over a small space using Kohonen maps following a deep architecture. The proposed architecture consists of a splitting process, layers of alternating self-organizing, a rectification function RELU and an abstraction layer (convolution-pooling). The self-organizing layer is composed of a few SOMs with each map focusing on modelling a local sub-region. The most winning neurons of each SOM are then organized in a second sampling layer to generate a new 2D map. In parallel to this transmission of the winning neurons, an abstraction of the data space is obtained after the convolution-pooling module. The ReLU is then applied. This treatment is applied more than once, changing the size of the splitting window and the displacement step on the reconstructed input image each time. In this way, local information is gathered to form more global information in the upper layers by applying each time a convolution filter of the level. The architecture of the Unsupervised Deep Self-Organizing Map is unique and retains the same principle of deep learning algorithms. This architecture can be very interesting in a Big Data environment for machine learning tasks. Experiments have been conducted to discuss how the proposed architecture shows this performance.},
	language = {en},
	number = {27-28},
	urldate = {2022-07-18},
	journal = {Multimedia Tools and Applications},
	author = {Sakkari, Mohamed and Zaied, Mourad},
	month = jul,
	year = {2020},
	pages = {19451--19470},
	file = {Sakkari and Zaied - 2020 - A Convolutional Deep Self-Organizing Map Feature e.pdf:/home/noemie/Zotero/storage/JPY9H23U/Sakkari and Zaied - 2020 - A Convolutional Deep Self-Organizing Map Feature e.pdf:application/pdf},
}

@inproceedings{wang_deep_2017,
	address = {Mountain View California USA},
	title = {Deep {Supervised} {Quantization} by {Self}-{Organizing} {Map}},
	isbn = {978-1-4503-4906-2},
	url = {https://dl.acm.org/doi/10.1145/3123266.3123415},
	doi = {10.1145/3123266.3123415},
	abstract = {Approximate Nearest Neighbour (ANN) search is an important research topic in multimedia and computer vision fields. In this paper, we propose a new deep supervised quantization method by Self-Organizing Map (SOM) to address this problem. Our method integrates the Convolutional Neural Networks (CNN) and SelfOrganizing Map into a unified deep architecture. The overall training objective includes supervised quantization loss and classification loss. With the supervised quantization loss, we minimize the differences on the maps between similar image pairs, and maximize the differences on the maps between dissimilar image pairs. By optimization, the deep architecture can simultaneously extract deep features and quantize the features into the suitable nodes in the Self-Organizing Map. The experiments on several public standard datasets prove the superiority of our approach over the existing ANN search methods. Besides, as a byproduct, our deep architecture can be directly applied to classification task and visualization with little modification, and promising performances are demonstrated on these tasks in the experiments.},
	language = {en},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 25th {ACM} international conference on {Multimedia}},
	publisher = {ACM},
	author = {Wang, Min and Zhou, Wengang and Tian, Qi and Pu, Junfu and Li, Houqiang},
	month = oct,
	year = {2017},
	pages = {1707--1715},
	file = {Wang et al. - 2017 - Deep Supervised Quantization by Self-Organizing Ma.pdf:/home/noemie/Zotero/storage/BHFS4WZT/Wang et al. - 2017 - Deep Supervised Quantization by Self-Organizing Ma.pdf:application/pdf},
}

@article{wickramasinghe_deep_2019,
	title = {Deep {Self}-{Organizing} {Maps} for {Unsupervised} {Image} {Classification}},
	volume = {15},
	issn = {1551-3203, 1941-0050},
	url = {https://ieeexplore.ieee.org/document/8669852/},
	doi = {10.1109/TII.2019.2906083},
	abstract = {The Deep Self-Organizing Map (DSOM) was introduced to embed hierarchical feature abstraction capability to SOMs. This paper presents an extended version of the original DSOM algorithm (E-DSOM). E-DSOM enhances the DSOM in two ways: 1) learning algorithm is modiﬁed to be completely unsupervised, and 2) architecture is modiﬁed to learn features of different resolution in hidden layers. E-DSOM has three main advantages over the original DSOM: 1) improved classiﬁcation accuracy, 2) improved generalization capability and 3) need of fewer sequential layers (reduced training time). E-DSOM was tested on benchmark and real-world datasets and was compared against DSOM, SOM, Stacked Autoencoder (AE), and Stacked Convolutional Autoencoder (CAE). Experimental results showed that the E-DSOM outperformed DSOM with improvements of classiﬁcation accuracy up to 15\% while saving training time up to 19\% on all datasets. Moreover, E-DSOM evidenced better generalization capability compared to the DSOM by showing superior performance on all datasets with induced noise. Further, E-DSOM showed comparable performance to the AE and the CAE while outperforming them on two datasets.},
	language = {en},
	number = {11},
	urldate = {2022-07-18},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Wickramasinghe, Chathurika S. and Amarasinghe, Kasun and Manic, Milos},
	month = nov,
	year = {2019},
	pages = {5837--5845},
	file = {Wickramasinghe et al. - 2019 - Deep Self-Organizing Maps for Unsupervised Image C.pdf:/home/noemie/Zotero/storage/YZWXVN4S/Wickramasinghe et al. - 2019 - Deep Self-Organizing Maps for Unsupervised Image C.pdf:application/pdf},
}

@article{zhao_stacked_2015,
	title = {Stacked {Multilayer} {Self}-{Organizing} {Map} for {Background} {Modeling}},
	volume = {24},
	issn = {1941-0042},
	doi = {10.1109/TIP.2015.2427519},
	abstract = {In this paper, a new background modeling method called stacked multilayer self-organizing map background model (SMSOM-BM) is proposed, which presents several merits such as strong representative ability for complex scenarios, easy to use, and so on. In order to enhance the representative ability of the background model and make the parameters learned automatically, the recently developed idea of representative learning (or deep learning) is elegantly employed to extend the existing single-layer self-organizing map background model to a multilayer one (namely, the proposed SMSOM-BM). As a consequence, the SMSOM-BM gains several merits including strong representative ability to learn background model of challenging scenarios, and automatic determination for most network parameters. More specifically, every pixel is modeled by a SMSOM, and spatial consistency is considered at each layer. By introducing a novel over-layer filtering process, we can train the background model layer by layer in an efficient manner. Furthermore, for real-time performance consideration, we have implemented the proposed method using NVIDIA CUDA platform. Comparative experimental results show superior performance of the proposed approach.},
	number = {9},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhao, Zhenjie and Zhang, Xuebo and Fang, Yongchun},
	month = sep,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Computational modeling, Biological neural networks, Training, Data models, self-organizing map, Training data, Adaptation models, Background modeling, Maintenance engineering, representative learning},
	pages = {2841--2850},
	file = {IEEE Xplore Abstract Record:/home/noemie/Zotero/storage/8WTWA5GK/7097028.html:text/html;IEEE Xplore Full Text PDF:/home/noemie/Zotero/storage/MM55LC2R/Zhao et al. - 2015 - Stacked Multilayer Self-Organizing Map for Backgro.pdf:application/pdf},
}

@article{nawaratne_hierarchical_2020-1,
	title = {Hierarchical {Two}-{Stream} {Growing} {Self}-{Organizing} {Maps} {With} {Transience} for {Human} {Activity} {Recognition}},
	volume = {16},
	issn = {1941-0050},
	doi = {10.1109/TII.2019.2957454},
	abstract = {The rapid growth in autonomous industrial environments has increased the need for intelligent video surveillance. As a predominant element of video surveillance, recognition of complex human movements is important in a wide range of surveillance applications. However, the current state-of-the-art video surveillance techniques use supervised deep learning pipelines for human activity recognition (HAR). A key shortcoming of such techniques is the inability to learn from unlabeled video streams. To operate effectively in natural environments, video surveillance techniques have to be able to handle huge volumes of unlabeled video data, monitor and generate alerts and insights derived from multiple characteristics such as spatial structure, motion flow, color distribution, etc. Furthermore, most conventional learning systems lack memory persistence capability which can reduce the influence of outdated information in memory-guided decision-making resulting in limiting plasticity and overfitting based on specific past events. In this article, we propose a new adaptation of the Growing Self-Organizing Map (GSOM) to address these shortcomings by 1) adopting two proven concepts of traditional deep learning, hierarchical, and multistream learning, applied into GSOM self-structuring architecture to accommodate learning from unlabeled video data and their diverse characteristics, 2) address overfitting and the influence of outdated information on neural architecture by implementing a transience property in the algorithm. We demonstrate the proposed model using three benchmark video datasets and the results confirm its validity and usability for HAR.},
	number = {12},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Nawaratne, Rashmika and Alahakoon, Damminda and De Silva, Daswin and Kumara, Harsha and Yu, Xinghuo},
	month = dec,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	keywords = {Neurons, self-organizing maps, Histograms, neural networks, Self-organizing feature maps, Feature extraction, Activity recognition, Forgetting, hierarchical learning, human activity recognition (HAR), Optical flow, Video surveillance},
	pages = {7756--7764},
	annote = {Modèle hiérarchique de carte. 
Applciation au traitment de vidéos.
question: sur quels données de sortie des cartes apprennent les couches sup ? on ne sait pas. 
},
	file = {IEEE Xplore Abstract Record:/home/noemie/Zotero/storage/28ML4TZD/8922590.html:text/html;IEEE Xplore Full Text PDF:/home/noemie/Zotero/storage/DAW47ADU/Nawaratne et al. - 2020 - Hierarchical Two-Stream Growing Self-Organizing Ma.pdf:application/pdf},
}

@article{miikkulainen_script_1992,
	title = {{SCRIPT} {RECOGNITION} {WITH} {HIERARCHICAL} {FEATURE} {MAPS}  y},
	volume = {2},
	abstract = {The hierarchical feature map system recognizes an input story as an instance of a particular script by classifying it at three levels: scripts, tracks and role bindings. The recognition taxonomy, i.e. the breakdown of each script into the tracks and roles, is extracted automatically and independently for each script from examples of script instantiations in an unsupervised self-organizing process. The process resembles human learning in that the di erentiation of the most frequently encountered scripts become gradually the most detailed. The resulting structure is a hierachical pyramid of feature maps. The hierarchy visualizes the taxonomy and the maps lay out the topology of each level. The number of input lines and the self-organization time are considerably reduced compared to the ordinary single-level feature mapping. The system can recognize incomplete stories and recover the missing events. The taxonomy also serves as memory organization for script-based episodic memory. The maps assign a unique memory location for each script instantiation. The most salient parts of the input data are separated and most resources are concentrated on representing them accurately.},
	language = {en},
	journal = {Connection Science},
	author = {Miikkulainen, Risto},
	year = {1992},
	pages = {196--214},
	file = {Miikkulainen - SCRIPT RECOGNITION WITH HIERARCHICAL FEATURE MAPS .pdf:/home/noemie/Zotero/storage/JX85R8FP/Miikkulainen - SCRIPT RECOGNITION WITH HIERARCHICAL FEATURE MAPS .pdf:application/pdf},
}

@misc{shinozaki_biologically_2017,
	title = {Biologically {Inspired} {Feedforward} {Supervised} {Learning} for {Deep} {Self}-{Organizing} {Map} {Networks}},
	url = {http://arxiv.org/abs/1710.09574},
	abstract = {In this study, we propose a novel deep neural network and its supervised learning method that uses a feedforward supervisory signal. The method is inspired by the human visual system and performs human-like association-based learning without any backward error propagation. The feedforward supervisory signal that produces the correct result is preceded by the target signal and associates its conﬁrmed label with the classiﬁcation result of the target signal. It eﬀectively uses a large amount of information from the feedforward signal, and forms a continuous and rich learning representation. The method is validated using visual recognition tasks on the MNIST handwritten dataset.},
	language = {en},
	urldate = {2022-07-19},
	publisher = {arXiv},
	author = {Shinozaki, Takashi},
	month = oct,
	year = {2017},
	note = {arXiv:1710.09574 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437)},
	file = {Shinozaki - 2017 - Biologically Inspired Feedforward Supervised Learn.pdf:/home/noemie/Zotero/storage/9M6HDX3C/Shinozaki - 2017 - Biologically Inspired Feedforward Supervised Learn.pdf:application/pdf},
}

@article{suganthan_pattern_2001,
	title = {Pattern classification using multiple hierarchical overlapped self-organising maps},
	abstract = {In this paper, we describe techniques for designing high-performance pattern classi"cation systems using multiple hierarchical overlapped self-organising maps (HOSOM) (Suganthan, Proceedings of the International Joint Conference on Neural Networks, WCCI'98, Alaska, 1998). The HOSOM model has one "rst level SOM and several partially overlapping second-level SOMs. With this overlap, every training and test sample is classi"ed by multiple second-level SOMs. Hence, the "nal classi"cation decision can be made by combining these multiple classi"cation decisions to obtain a better performance. In this paper, we use multiple HOSOMs and each HOSOM is trained on a distinct input feature set extracted from the same data set. Since one HOSOM yields multiple classi"cations, these multiple HOSOMs generate a large number of classi"cation decisions. To combine the individual classi"cations, we make use of the global winner as well as a winner for every class. Our experiments yielded a high recognition rate of 99.25\% on NIST19 numeral database. 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
	language = {en},
	journal = {Pattern Recognition},
	author = {Suganthan, P N},
	year = {2001},
	pages = {7},
	file = {Suganthan - 2001 - Pattern classication using multiple hierarchical .pdf:/home/noemie/Zotero/storage/ZXG99UWK/Suganthan - 2001 - Pattern classication using multiple hierarchical .pdf:application/pdf},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2022-07-20},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/home/noemie/Zotero/storage/GHBQRVIW/LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}
@inproceedings{Kohonen1991THEHA,
  title={THE HYPERMAP ARCHITECTURE},
  author={Teuvo Kohonen},
  year={1991}
}

@article{johnsson_associating_2008,
	title = {Associating {SOM} {Representations} of {Haptic} {Submodalities}},
	abstract = {We have experimented with a bio-inspired selforganizing texture and hardness perception system which automatically learns to associate the representations of the two submodalities with each other. To this end we have developed a microphone based texture sensor and a hardness sensor that measures the compression of the material at a constant pressure. The system is based on a novel variant of the Self-Organizing Map (SOM), called Associative Self-Organizing Map (A-SOM). The A-SOM both develops a representation of its input space and learns to associate this with the activity in an external SOM or A-SOM. The system was trained and tested with multiple samples gained from the exploration of a set of 4 soft and 4 hard objects of different materials with varying textural properties. The system successfully found representations of the texture and hardness submodalities and also learned to associate these with each other.},
	language = {en},
	date = {2008},
	journal={Computer Science},
	author = {Johnsson, Magnus and Balkenius, Christian},
	pages = {6},
	file = {Johnsson and Balkenius - Associating SOM Representations of Haptic Submodal.pdf:/home/noemie/Zotero/storage/XH3IP7CJ/Johnsson and Balkenius - Associating SOM Representations of Haptic Submodal.pdf:application/pdf},
}

@article{primate_cortex_91,
title={Distributed Hierarchical Processing
in the Primate Cerebral Cortex},
author={Daniel J. Felleman and David C. Van Essen},
year = {1991},
booktitle={Cerebral Cortex}
}
@article{mountcastle_columnar_1997,
	title = {The columnar organization of the neocortex},
	volume = {120},
	issn = {14602156},
	url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/120.4.701},
	doi = {10.1093/brain/120.4.701},
	language = {en},
	number = {4},
	urldate = {2021-07-21},
	journal = {Brain},
	author = {Mountcastle, V.},
	month = apr,
	year = {1997},
	pages = {701--722},
	file = {Mountcastle - 1997 - The columnar organization of the neocortex.pdf:/home/noemie/Zotero/storage/YDSLD6ME/Mountcastle - 1997 - The columnar organization of the neocortex.pdf:application/pdf},
}

@InCollection{binzegger05,
   author =      {Tom Binzegger and Rodney J. Douglas and Kevan A. C.
Martin},
   title =      {Cortical Architecture},
   booktitle =      {Brain, Vision, and Artifical Intelligence},
   publisher = {Springer-Verlag},
   year =      {2005},
   editor =      {M. {De Gregorio} and V. {Di Maio} and M. Frucci and C.
Musio}
}

@article{Meunier2010ModularAH,
  title={Modular and Hierarchically Modular Organization of Brain Networks},
  author={D. Meunier and R. Lambiotte and E. Bullmore},
  journal={Frontiers in Neuroscience},
  year={2010},
  volume={4}
}


@inproceedings{lefort_active_2015,
	title = {Active learning of local predictable representations with artificial curiosity},
	booktitle = {2015 {Joint} {IEEE} {International} {Conference} on {Development} and {Learning} and {Epigenetic} {Robotics} ({ICDL}-{EpiRob})},
	author = {Lefort, M. and Gepperth, A.},
	year = {2015}
}


@inproceedings{johnsson_associative_2009,
	title = {Associative Self-Organizing Map},
	author = {M. Johnsson and C. Balkenius and G. Hesslow},
	year = {2009},
	booktitle = {Proc. IJCCI}
	}
	

@article{Liu2015DeepSM,
  title={Deep Self-Organizing Map for visual classification},
  author={N. Liu and Jinjun Wang and Yihong Gong},
  journal={2015 International Joint Conference on Neural Networks (IJCNN)},
  year={2015}
}

@ARTICLE{parisiLL,
	  
	AUTHOR={Parisi, German I. and Tani, Jun and Weber, Cornelius and Wermter, Stefan},   
		 
	TITLE={Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization},      
		
	JOURNAL={Frontiers in Neurorobotics}, 
		
	YEAR={2018}
	}
	
@inproceedings{baheux_towards_2014,
  TITLE = {{Towards an effective multi-map self organizing recurrent neural network}},
  AUTHOR = {D. Baheux and J. Fix and H. Frezza-Buet},
  BOOKTITLE = {Proc. ESANN},
  YEAR = {2014},
}


@article{khouzam,
  TITLE = {{Distributed recurrent self-organization for tracking the state of non-stationary partially observable dynamical systems}},
  AUTHOR = {B. Khouzam and H.Frezza-Buet},
  JOURNAL = {{Biologically Inspired Cognitive Architectures}},
  YEAR = {2013}
  }


@article{dominey13,
author = {Lallee, Stephane and Dominey, Peter},
year = {2013},
pages = {274--285},
title = {Multi-modal convergence maps: From body schema and self-representation to mental imagery},
volume = {21},
number = {4},
journal = {Adaptive Behavior},
}

@inproceedings{ASOM,
  author       = {Johnsson, Magnus and Balkenius, Christian and Hesslow, Germund},
  booktitle    = {Proc. IJCCI'09},
  pages        = {363--370},
  title        = {Associative Self-Organizing Map},
  year         = {2009}
}

@ARTICLE{SOIMA,
  
AUTHOR={Escobar-Juárez, Esau and Schillaci, Guido and Hermosillo-Valadez, Jorge and Lara-Guzmán, Bruno},   
	 
TITLE={A Self-Organized Internal Models Architecture for Coding Sensory–Motor Schemes},      
	
JOURNAL={Frontiers in Robotics and AI},      
	
VOLUME={3},      
	
YEAR={2016},      
	  
URL={https://www.frontiersin.org/article/10.3389/frobt.2016.00022},       
	
DOI={10.3389/frobt.2016.00022},      
	
ISSN={2296-9144},   

}


@article{varsta_temporal_2001,
	title = {Temporal {Kohonen} {Map} and the {Recurrent} {Self}-{Organizing} {Map}: {Analytical} and {Experimental} {Comparison}},
	language = {en},
	author = {Varsta, Markus and Heikkonen, Jukka and Lampinen, Jouko},
	pages = {15},
	journal = {neural processing letters},
	year = {2001}
}


@inproceedings{hammer_self-organizing_2005,
	title = {Self-{Organizing} {Maps} for {Time} {Series}},
	language = {en},
	author = {Hammer, Barbara and Micheli, Alessio and Neubauer, Nicolas and Sperduti, Alessandro and Strickert, Marc},
	year = {2005},
	pages = {8},
	booktitle = {Proc. WSOM'2005}
}

@incollection{fix20,
  TITLE = {{Look and Feel What and How Recurrent Self-Organizing Maps Learn}},
  AUTHOR = {Fix, J{\'e}r{\'e}my and Frezza-Buet, Herv{\'e}},
  BOOKTITLE = {Proc. WSOM'19},
  VOLUME = {976},
  PAGES = {3--12},
  YEAR = {2020}
}

@article{hammer_recursive_2004,
	title = {Recursive self-organizing network models},
	volume = {17},
	issn = {08936080},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608004001510},
	doi = {10.1016/j.neunet.2004.06.009},
	language = {en},
	number = {8-9},
	urldate = {2018-11-07},
	journal = {Neural Networks},
	author = {Hammer, Barbara and Micheli, Alessio and Sperduti, Alessandro and Strickert, Marc},
	month = oct,
	year = {2004},
	pages = {1061--1085}
}

@incollection{KOHONEN19911357,
title = {THE HYPERMAP ARCHITECTURE},
editor = {Teuvo KOHONEN and Kai MÄKISARA and Olli SIMULA and Jari KANGAS},
booktitle = {Artificial Neural Networks},
publisher = {North-Holland},
address = {Amsterdam},
pages = {1357-1360},
year = {1991},
isbn = {978-0-444-89178-5},
doi = {https://doi.org/10.1016/B978-0-444-89178-5.50088-9},author = {Teuvo Kohonen}}
}

@article{Chappell1993TheTK,
  title={The temporal Koh{\"o}nen map},
  author={Geoffrey J. Chappell and John G. Taylor},
  journal={Neural Networks},
  year={1993},
  volume={6},
  pages={441-445}
}

@article{Voegtlin2002RecursiveSM,
  title={Recursive self-organizing maps},
  author={Thomas Voegtlin},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2002},
  volume={15 8-9},
  pages={979-91 }
}

@article{Strickert2005MergeSF,
  title={Merge SOM for temporal data},
  author={Marc Strickert and Barbara Hammer},
  journal={Neurocomputing},
  year={2005},
  volume={64},
  pages={39-71}
}

@article{menard05,
  TITLE = {{Model of multi-modal cortical processing: Coherent learning in self-organizing modules}},
  AUTHOR = {M{\'e}nard, Olivier and Frezza-Buet, Herv{\'e}},
  JOURNAL = {{Neural Networks}},
  PUBLISHER = {{Elsevier}},
  VOLUME = {18},
  NUMBER = {5-6},
  PAGES = {646--655},
  YEAR = {2005}
}


@article{damasio_time-locked_1989,
	title = {Time-locked multiregional retroactivation: {A} systems-level proposal for the neural substrates of recall and recognition},
	volume = {33},
	issn = {0010-0277},
	url = {https://www.sciencedirect.com/science/article/pii/001002778990005X},
	doi = {https://doi.org/10.1016/0010-0277(89)90005-X},
	
	number = {1},
	journal = {Cognition},
	author = {Damasio, Antonio R.},
	year = {1989},
	pages = {25--62},
	annote = {Special Issue Neurobiology of Cognition},
}

@inproceedings{Edelman1982GroupSA,
  title={Group selection and phasic reentrant signaling a theory of higher brain function},
  author={Gerald M. Edelman},
  year={1982},
  booktitle={The 4th Intensive Study Program
Of The Neurosciences Research Program} 
}


@article{hagenbuchner_self-organizing_2003,
	title = {A self-organizing map for adaptive processing of structured data},
	volume = {14},
	issn = {1045-9227},
	url = {http://ieeexplore.ieee.org/document/1199648/},
	doi = {10.1109/TNN.2003.810735},
	abstract = {Recent developments in the area of neural networks produced models capable of dealing with structured data. Here, we propose the first fully unsupervised model, namely an extension of traditional self-organizing maps (SOMs), for the processing of labeled directed acyclic graphs (DAGs). The extension is obtained by using the unfolding procedure adopted in recurrent and recursive neural networks, with the replicated neurons in the unfolded network comprising of a full SOM. This approach enables the discovery of similarities among objects including vectors consisting of numerical data. The capabilities of the model are analyzed in detail by utilizing a relatively large data set taken from an artificial benchmark problem involving visual patterns encoded as labeled DAGs. The experimental results demonstrate clearly that the proposed model is capable of exploiting both information conveyed in the labels attached to each node of the input DAGs and information encoded in the DAG topology.},
	language = {en},
	number = {3},
	urldate = {2018-12-10},
	journal = {IEEE Transactions on Neural Networks},
	author = {Hagenbuchner, M. and Sperduti, A. and {Ah Chung Tsoi}},
	month = may,
	year = {2003},
	pages = {491--505}
}

@Article{Kohonen1982,
author={Kohonen, Teuvo},
title={Self-organized formation of topologically correct feature maps},
journal={Biological Cybernetics},
year={1982},
volume={43},
number={1},
pages={59--69},
}

@inproceedings{Kohonen1995SelfOrganizingM,
  title={Self-Organizing Maps},
  author={Teuvo Kohonen},
  booktitle={Springer Series in Information Sciences},
  year={1995}
}

@incollection{cottrell_theoretical_2016,
	address = {Cham},
	title = {Theoretical and {Applied} {Aspects} of the {Self}-{Organizing} {Maps}},
	volume = {428},
	isbn = {978-3-319-28517-7 978-3-319-28518-4},
	abstract = {The Self-Organizing Map (SOM) is widely used, easy to implement, has nice properties for data mining by providing both clustering and visual representation. It acts as an extension of the k-means algorithm that preserves as much as possible the topological structure of the data. However, since its conception, the mathematical study of the SOM remains diﬃcult and has be done only in very special cases. In WSOM 2005, Jean-Claude Fort presented the state of the art, the main remaining diﬃculties and the mathematical tools that can be used to obtain theoretical results on the SOM outcomes. These tools are mainly Markov chains, the theory of Ordinary Diﬀerential Equations, the theory of stability, etc. This article presents theoretical advances made since then. In addition, it reviews some of the many SOM algorithm variants which were deﬁned to overcome the theoretical diﬃculties and/or adapt the algorithm to the processing of complex data such as time series, missing values in the data, nominal data, textual data, etc.},
	language = {en},
	urldate = {2018-12-17},
	booktitle = {Advances in {Self}-{Organizing} {Maps} and {Learning} {Vector} {Quantization}},
	publisher = {Springer International Publishing},
	author = {Cottrell, Marie and Olteanu, Madalina and Rossi, Fabrice and Villa-Vialaneix, Nathalie},
	year = {2016},
	doi = {10.1007/978-3-319-28518-4_1},
	pages = {3--26}
}


@article{khacef_brain-inspired_2020,
	title = {Brain-inspired self-organization with cellular neuromorphic computing for multimodal unsupervised learning},
	url = {http://arxiv.org/abs/2004.05488},
	abstract = {Cortical plasticity is one of the main features that enable our capability to learn and adapt in our environment. Indeed, the cerebral cortex has the ability to self-organize itself through two distinct forms of plasticity: the structural plasticity that creates (sprouting) or cuts (pruning) synaptic connections between neurons, and the synaptic plasticity that modiﬁes the synaptic connections strength. These mechanisms are very likely at the basis of an extremely interesting characteristic of the human brain development: the multimodal association. The brain uses spatio-temporal correlations between several modalities to structure the data and create sense from observations. Thus, in spite of the diversity of the sensory modalities, like sight, sound and touch, the brain arrives at the same concepts. Moreover, biological observations show that one modality can activate the internal representation of another modality when both are correlated. To model such a behavior, Edelman and Damasio proposed respectively the Reentry and the Convergence Divergence Zone frameworks where bi-directional neural communications can lead to both multimodal fusion (convergence) and inter-modal activation (divergence). Nevertheless, these frameworks do not provide a computational model at the neuron level, and only few papers tackle this issue of bio-inspired multimodal association which is yet necessary for a complete representation of the environment. In this paper, we build a brain-inspired neural system based on the Reentry principles, using Self-Organizing Maps and Hebbian-like learning. We propose and compare different computational methods for unsupervised learning and inference, then quantify the gain of both convergence and divergence mechanisms in a multimodal classiﬁcation task. The divergence mechanism is used to label one modality based on the other, while the convergence mechanism is used to improve the overall accuracy of the system. We perform our experiments on a constructed written/spoken digits database and a DVS/EMG hand gestures database. Finally, we implement our system on the Iterative Grid, a cellular neuromorphic architecture that enables distributed computing with local connectivity. We show the gain of the so-called hardware plasticity induced by our model, where the system’s topology is not ﬁxed by the user but learned along the system’s experience through self-organization.},
	language = {en},
	urldate = {2020-08-31},
	journal = {arXiv:2004.05488 [cs, q-bio]},
	author = {Khacef, Lyes and Rodriguez, Laurent and Miramond, Benoit},
	month = jun,
	year = {2020},
	note = {arXiv: 2004.05488},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: Preprint, 24 pages, 11 figures, 4 tables},
	file = {Khacef et al. - 2020 - Brain-inspired self-organization with cellular neu.pdf:/home/noemie/Zotero/storage/TS8SAMRN/Khacef et al. - 2020 - Brain-inspired self-organization with cellular neu.pdf:application/pdf},
}


@article{johnsson_associating_2008b,
	title = {Associating {SOM} {Representations} of {Haptic} {Submodalities}},
	abstract = {We have experimented with a bio-inspired selforganizing texture and hardness perception system which automatically learns to associate the representations of the two submodalities with each other. To this end we have developed a microphone based texture sensor and a hardness sensor that measures the compression of the material at a constant pressure. The system is based on a novel variant of the Self-Organizing Map (SOM), called Associative Self-Organizing Map (A-SOM). The A-SOM both develops a representation of its input space and learns to associate this with the activity in an external SOM or A-SOM. The system was trained and tested with multiple samples gained from the exploration of a set of 4 soft and 4 hard objects of different materials with varying textural properties. The system successfully found representations of the texture and hardness submodalities and also learned to associate these with each other.},
	language = {en},
	author = {Johnsson, Magnus and Balkenius, Christian},
	pages = {6},
	date = {2008}
}

@inproceedings{Buonamente2013SimulatingAW,
  title={Simulating Actions with the Associative Self-Organizing Map},
  author={Miriam Buonamente and Haris Dindo and Magnus Johnsson},
  booktitle={AIC@AI*IA},
  year={2013}
}

@article{buonamente_hierarchies_2016,
	title = {Hierarchies of {Self}-{Organizing} {Maps} for action recognition},
	volume = {39},
	issn = {13890417},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S138904171600005X},
	doi = {10.1016/j.cogsys.2015.12.009},
	abstract = {We propose a hierarchical neural architecture able to recognise observed human actions. Each layer in the architecture represents increasingly complex human activity features. The ﬁrst layer consists of a SOM which performs dimensionality reduction and clustering of the feature space. It represents the dynamics of the stream of posture frames in action sequences as activity trajectories over time. The second layer in the hierarchy consists of another SOM which clusters the activity trajectories of the ﬁrst-layer SOM and learns to represent action prototypes. The third- and last-layer of the hierarchy consists of a neural network that learns to label action prototypes of the second-layer SOM and is independent – to certain extent – of the camera’s angle and relative distance to the actor. The experiments were carried out with encouraging results with action movies taken from the INRIA 4D repository. In terms of representational accuracy, measured as the recognition rate over the training set, the architecture exhibits 100\% accuracy indicating that actions with overlapping patterns of activity can be correctly discriminated. On the other hand, the architecture exhibits 53\% recognition rate when presented with the same actions interpreted and performed by a diﬀerent actor. Experiments on actions captured from diﬀerent view points revealed a robustness of our system to camera rotation. Indeed, recognition accuracy was comparable to the single viewpoint case. To further assess the performance of the system we have also devised a behavioural experiments in which humans were asked to recognise the same set of actions, captured from diﬀerent points of view. Results form such a behavioural study let us argue that our architecture is a good candidate as cognitive model of human action recognition, as architectural results are comparable to those observed in humans.},
	language = {en},
	urldate = {2022-07-25},
	journal = {Cognitive Systems Research},
	author = {Buonamente, Miriam and Dindo, Haris and Johnsson, Magnus},
	month = sep,
	year = {2016},
	pages = {33--41},
	file = {Buonamente et al. - 2016 - Hierarchies of Self-Organizing Maps for action rec.pdf:/home/noemie/Zotero/storage/PQGPCYZY/Buonamente et al. - 2016 - Hierarchies of Self-Organizing Maps for action rec.pdf:application/pdf},
}

% Contexte, systèmes complexes

@article{Horvth2009AnED,
  title={An Experimental Design Method Leading to Chemical Turing Patterns},
  author={Judit Horv{\'a}th and Istv{\'a}n Szalai and Patrick De Kepper},
  journal={Science},
  year={2009},
  volume={324},
  pages={772 - 775}
}

@article{Milgram1967TheSW,
  title={The Small World Problem},
  author={S Milgram},
  journal={Psychology today},
  year={1967},
  volume={2},
  pages={60-67}
}

@article{Clauset2008HierarchicalSA,
  title={Hierarchical structure and the prediction of missing links in networks},
  author={Aaron Clauset and Cristopher Moore and Mark E. J. Newman},
  journal={Nature},
  year={2008},
  volume={453},
  pages={98-101}
}

@article{Ravasz2002HierarchicalOO,
  title={Hierarchical Organization of Modularity in Metabolic Networks},
  author={Erzs{\'e}bet Ravasz and Audrey Somera and D A Mongru and Zolt{\'a}n N. Oltvai and A.-L. Barabasi},
  journal={Science},
  year={2002},
  volume={297},
  pages={1551 - 1555}
}

@article{Telesford2011TheUO,
  title={The Ubiquity of Small-World Networks},
  author={Qawi K. Telesford and Karen E. Joyce and Satoru Hayasaka and Jonathan H. Burdette and Paul J. Laurienti},
  journal={Brain connectivity},
  year={2011},
  volume={1 5},
  pages={
          367-75
        }
}

@article{complex_adaptative_systemes,
 ISSN = {00115266},
 author = {John H. Holland},
 journal = {Daedalus},
 number = {1},
 pages = {17--30},
 publisher = {The MIT Press},
 title = {Complex Adaptive Systems},
 volume = {121},
 year = {1992}
}

%Inspiration biologique

@article{primate_cortex_91,
title={Distributed Hierarchical Processing
in the Primate Cerebral Cortex},
author={Daniel J. Felleman and David C. Van Essen},
year = {1991},
booktitle={Cerebral Cortex}
}

@article{turing52,
 ISSN = {00804622},
 author = {A. M. Turing},
 journal = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
 number = {641},
 pages = {37--72},
 publisher = {The Royal Society},
 title = {The Chemical Basis of Morphogenesis},
 volume = {237},
 year = {1952}
}

@article{WANDELL2007366,
title = {Visual Field Maps in Human Cortex},
journal = {Neuron},
volume = {56},
number = {2},
pages = {366-383},
year = {2007},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2007.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S089662730700774X},
author = {Brian A. Wandell and Serge O. Dumoulin and Alyssa A. Brewer}

}

@inproceedings{Morowitz1995TheMT,
  title={The Mind, the Brain, and Complex Adaptive Systems},
  author={Harold J. Morowitz},
  year={1995}
}

@inproceedings{Rolls2002ComputationalNO,
  title={Computational neuroscience of vision},
  author={Edmund T. Rolls and Gustavo Deco},
  year={2002}
}

@article{mountcastle_columnar_1997,
	title = {The columnar organization of the neocortex},
	volume = {120},
	issn = {14602156},
	url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/120.4.701},
	doi = {10.1093/brain/120.4.701},
	language = {en},
	number = {4},
	urldate = {2021-07-21},
	journal = {Brain},
	author = {Mountcastle, V.},
	month = apr,
	year = {1997},
	pages = {701--722},
	file = {Mountcastle - 1997 - The columnar organization of the neocortex.pdf:/home/noemie/Zotero/storage/YDSLD6ME/Mountcastle - 1997 - The columnar organization of the neocortex.pdf:application/pdf},
}

@InCollection{binzegger05,
   author =      {Tom Binzegger and Rodney J. Douglas and Kevan A. C.
Martin},
   title =      {Cortical Architecture},
   booktitle =      {Brain, Vision, and Artifical Intelligence},
   publisher = {Springer-Verlag},
   year =      {2005},
   editor =      {M. {De Gregorio} and V. {Di Maio} and M. Frucci and C.
Musio}
}

@article{Meunier2010ModularAH,
  title={Modular and Hierarchically Modular Organization of Brain Networks},
  author={D. Meunier and R. Lambiotte and E. Bullmore},
  journal={Frontiers in Neuroscience},
  year={2010},
  volume={4}
}

@article{Harriger2012RichCO,
  title={Rich Club Organization of Macaque Cerebral Cortex and Its Role in Network Communication},
  author={Logan Harriger and Martijn P. van den Heuvel and Olaf Sporns},
  journal={PLoS ONE},
  year={2012},
  volume={7}
}


@article{biyu_scale-free_2014,
	title = {Scale-free brain activity: past, present, and future},
	volume = {18},
	number = {9},
	journal = {Trends in Cognitive Sciences},
	author = {Biyu, J. He},
	month = sep,
	year = {2014},
	file = {Biyu - 2014 - Scale-free brain activity past, present, and futu.pdf:/home/noemie/Zotero/storage/2YQAPIW2/Biyu - 2014 - Scale-free brain activity past, present, and futu.pdf:application/pdf},
}

@article{Barabasi2003ScaleFreeN,
  title={Scale-Free Networks.},
  author={A. L. Barabasi and Eric Bonabeau},
  journal={Scientific American},
  year={2003},
  volume={288},
  pages={60-69}
}
% Aspect computationel 

@article{towards_novel_2001,
title={Towards Novel Neuroscience-Inspired
Computing},
author={Stefan Wermter and Jim Austin and David Willshaw and Mark Elshaw},
year = {2001},
booktitle={Emergent Neural Computational Architectures, LNAI}
}


@article{brooks_sumsumption_85,
  title={A robust layered control system for a mobile robot},
  author={Rodney A. Brooks},
  journal={IEEE J. Robotics Autom.},
  year={1986},
  volume={2},
  pages={14-23}
}

@article{Siebert2020RoleOM,
  title={Role of modularity in self-organization dynamics in biological networks.},
  author={Bram A. Siebert and C. Hall and J. Gleeson and Malbor Asllani},
  journal={Physical review. E},
  year={2020},
  volume={102 5-1},
  pages={
          052306
        }
}

@inproceedings{Bryson2001ModularityAS,
  title={Modularity and Specialized Learning: Mapping between Agent Architectures and Brain Organization},
  author={J. Bryson and L. Stein},
  booktitle={Emergent Neural Computational Architectures Based on Neuroscience},
  year={2001}
}

@article{Pan2009ModularityPS,
  title={Modularity produces small-world networks with dynamical time-scale separation},
  author={Raj Kumar Pan and Sitabhra Sinha},
  journal={EPL},
  year={2009},
  volume={85},
  pages={68006}
}

%Exemples d'architectures modulaires 

@inproceedings{lefort_active_2015,
	title = {Active learning of local predictable representations with artificial curiosity},
	booktitle = {2015 {Joint} {IEEE} {International} {Conference} on {Development} and {Learning} and {Epigenetic} {Robotics} ({ICDL}-{EpiRob})},
	author = {Lefort, M. and Gepperth, A.},
	year = {2015}
}


@inproceedings{johnsson_associative_2009,
	title = {Associative Self-Organizing Map},
	author = {M. Johnsson and C. Balkenius and G. Hesslow},
	year = {2009},
	booktitle = {Proc. IJCCI}
	}
	
@article{LampinenClusteringPO,
  title={Clustering properties of hierarchical self-organizing maps},
  author={J. Lampinen and E. Oja},
  journal={Journal of Mathematical Imaging and Vision},
  year={1992}
}

@article{Liu2015DeepSM,
  title={Deep Self-Organizing Map for visual classification},
  author={N. Liu and Jinjun Wang and Yihong Gong},
  journal={2015 International Joint Conference on Neural Networks (IJCNN)},
  year={2015}
}

@ARTICLE{parisiLL,
	  
	AUTHOR={Parisi, German I. and Tani, Jun and Weber, Cornelius and Wermter, Stefan},   
		 
	TITLE={Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization},      
		
	JOURNAL={Frontiers in Neurorobotics}, 
		
	YEAR={2018}
	}
	
@inproceedings{baheux_towards_2014,
  TITLE = {{Towards an effective multi-map self organizing recurrent neural network}},
  AUTHOR = {D. Baheux and J. Fix and H. Frezza-Buet},
  BOOKTITLE = {Proc. ESANN},
  YEAR = {2014},
}


@article{khouzam,
  TITLE = {{Distributed recurrent self-organization for tracking the state of non-stationary partially observable dynamical systems}},
  AUTHOR = {B. Khouzam and H.Frezza-Buet},
  JOURNAL = {{Biologically Inspired Cognitive Architectures}},
  YEAR = {2013}
  }
  
% Du coté du deep learning ? 

@article{Andreas2016NeuralMN,
  title={Neural Module Networks},
  author={Jacob Andreas and Marcus Rohrbach and Trevor Darrell and D. Klein},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={39-48}
}

@inproceedings{Kirsch2018ModularNL,
  title={Modular Networks: Learning to Decompose Neural Computation},
  author={Louis Kirsch and Julius Kunze and D. Barber},
  booktitle={NeurIPS},
  year={2018}
}

@article{Watanabe2018ModularRO,
  title={Modular representation of layered neural networks},
  author={C. Watanabe and Kaoru Hiramatsu and K. Kashino},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2018},
  volume={97},
  pages={
          62-73
        }
}

@article{Csordas2021AreNN,
  title={Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks},
  author={Robert Csordas and Sjoerd van Steenkiste and J. Schmidhuber},
  journal={ArXiv},
  year={2021},
  volume={abs/2010.02066}
}

@article{Hilgetag2015IsTB,
  title={Is the brain really a small-world network?},
  author={Claus C. Hilgetag and Alexandros Goulas},
  journal={Brain Structure \& Function},
  year={2015},
  volume={221},
  pages={2361 - 2366}
}

@article{parisi17,
title = {Emergence of multimodal action representations from neural network self-organization},
journal = {Cognitive Systems Research},
volume = "43",
pages = {208--221},
year = "2017",
author = {German I. Parisi and Jun Tani and Cornelius Weber and Stefan Wermter}
}

@Article{electronics9101605,
AUTHOR = {Khacef, Lyes and Rodriguez, Laurent and Miramond, Benoît},
TITLE = {Brain-Inspired Self-Organization with Cellular Neuromorphic Computing for Multimodal Unsupervised Learning},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {10},
ARTICLE-NUMBER = {1605},
URL = {https://www.mdpi.com/2079-9292/9/10/1605},
ISSN = {2079-9292},
DOI = {10.3390/electronics9101605}
}

@inproceedings{koikkalainen_self-organizing_1990,
	title = {Self-organizing hierarchical feature maps},
	doi = {10.1109/IJCNN.1990.137727},
	booktitle = {1990 {IJCNN} {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Koikkalainen, P. and Oja, E.},
	month = jun,
	year = {1990},
	pages = {279--284 vol.2}
}

@article{lampinen_clustering_1992,
	title = {Clustering {Properties} of {Hierarchical} {Self}-{Organizing} {Maps}},
	language = {en},
	author = {Lampinen, Jouko and Oja, Erkki},
	pages = {15},
	year = {1992},
	journal = {Journal of Mathematical Imaging and Vision}
}

@inproceedings{Forrest1990EmergentCS,
  title={Emergent computation: self-organizing, collective, and cooperative phenomena in natural and artificial computing networks},
  author={Stephanie Forrest},
  year={1990}
}

@inproceedings{Bonabeau1999SwarmI,
  title={Swarm Intelligence - From Natural to Artificial Systems},
  author={Eric Bonabeau and Marco Dorigo and Guy Theraulaz},
  booktitle={Studies in the sciences of complexity},
  year={1999}
}



@article{dominey13,
author = {Lallee, Stephane and Dominey, Peter},
year = {2013},
pages = {274--285},
title = {Multi-modal convergence maps: From body schema and self-representation to mental imagery},
volume = {21},
number = {4},
journal = {Adaptive Behavior},
}

@inproceedings{ASOM,
  author       = {Johnsson, Magnus and Balkenius, Christian and Hesslow, Germund},
  booktitle    = {Proc. IJCCI'09},
  pages        = {363--370},
  title        = {Associative Self-Organizing Map},
  year         = {2009}
}

@ARTICLE{SOIMA,
  
AUTHOR={Escobar-Juárez, Esau and Schillaci, Guido and Hermosillo-Valadez, Jorge and Lara-Guzmán, Bruno},   
	 
TITLE={A Self-Organized Internal Models Architecture for Coding Sensory–Motor Schemes},      
	
JOURNAL={Frontiers in Robotics and AI},      
	
VOLUME={3},      
	
YEAR={2016},      
	  
URL={https://www.frontiersin.org/article/10.3389/frobt.2016.00022},       
	
DOI={10.3389/frobt.2016.00022},      
	
ISSN={2296-9144},   

}


@article{varsta_temporal_2001,
	title = {Temporal {Kohonen} {Map} and the {Recurrent} {Self}-{Organizing} {Map}: {Analytical} and {Experimental} {Comparison}},
	language = {en},
	author = {Varsta, Markus and Heikkonen, Jukka and Lampinen, Jouko},
	pages = {15},
	journal = {neural processing letters},
	year = {2001}
}


@inproceedings{hammer_self-organizing_2005,
	title = {Self-{Organizing} {Maps} for {Time} {Series}},
	language = {en},
	author = {Hammer, Barbara and Micheli, Alessio and Neubauer, Nicolas and Sperduti, Alessandro and Strickert, Marc},
	year = {2005},
	pages = {8},
	booktitle = {Proc. WSOM'2005}
}

@incollection{fix20,
  TITLE = {{Look and Feel What and How Recurrent Self-Organizing Maps Learn}},
  AUTHOR = {Fix, J{\'e}r{\'e}my and Frezza-Buet, Herv{\'e}},
  BOOKTITLE = {Proc. WSOM'19},
  VOLUME = {976},
  PAGES = {3--12},
  YEAR = {2020}
}

@article{hammer_recursive_2004,
	title = {Recursive self-organizing network models},
	volume = {17},
	issn = {08936080},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608004001510},
	doi = {10.1016/j.neunet.2004.06.009},
	language = {en},
	number = {8-9},
	urldate = {2018-11-07},
	journal = {Neural Networks},
	author = {Hammer, Barbara and Micheli, Alessio and Sperduti, Alessandro and Strickert, Marc},
	month = oct,
	year = {2004},
	pages = {1061--1085}
}

@incollection{KOHONEN19911357,
title = {THE HYPERMAP ARCHITECTURE},
editor = {Teuvo KOHONEN and Kai MÄKISARA and Olli SIMULA and Jari KANGAS},
booktitle = {Artificial Neural Networks},
publisher = {North-Holland},
address = {Amsterdam},
pages = {1357-1360},
year = {1991},
isbn = {978-0-444-89178-5},
doi = {https://doi.org/10.1016/B978-0-444-89178-5.50088-9},author = {Teuvo Kohonen}}
}

@article{Chappell1993TheTK,
  title={The temporal Koh{\"o}nen map},
  author={Geoffrey J. Chappell and John G. Taylor},
  journal={Neural Networks},
  year={1993},
  volume={6},
  pages={441-445}
}

@article{Voegtlin2002RecursiveSM,
  title={Recursive self-organizing maps},
  author={Thomas Voegtlin},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2002},
  volume={15 8-9},
  pages={979-91 }
}

@article{Strickert2005MergeSF,
  title={Merge SOM for temporal data},
  author={Marc Strickert and Barbara Hammer},
  journal={Neurocomputing},
  year={2005},
  volume={64},
  pages={39-71}
}

@article{menard05,
  TITLE = {{Model of multi-modal cortical processing: Coherent learning in self-organizing modules}},
  AUTHOR = {M{\'e}nard, Olivier and Frezza-Buet, Herv{\'e}},
  JOURNAL = {{Neural Networks}},
  PUBLISHER = {{Elsevier}},
  VOLUME = {18},
  NUMBER = {5-6},
  PAGES = {646--655},
  YEAR = {2005}
}


@article{damasio_time-locked_1989,
	title = {Time-locked multiregional retroactivation: {A} systems-level proposal for the neural substrates of recall and recognition},
	volume = {33},
	issn = {0010-0277},
	url = {https://www.sciencedirect.com/science/article/pii/001002778990005X},
	doi = {https://doi.org/10.1016/0010-0277(89)90005-X},
	
	number = {1},
	journal = {Cognition},
	author = {Damasio, Antonio R.},
	year = {1989},
	pages = {25--62},
	annote = {Special Issue Neurobiology of Cognition},
}

@inproceedings{Edelman1982GroupSA,
  title={Group selection and phasic reentrant signaling a theory of higher brain function},
  author={Gerald M. Edelman},
  year={1982},
  booktitle={The 4th Intensive Study Program
Of The Neurosciences Research Program} 
}


@article{hagenbuchner_self-organizing_2003,
	title = {A self-organizing map for adaptive processing of structured data},
	volume = {14},
	issn = {1045-9227},
	url = {http://ieeexplore.ieee.org/document/1199648/},
	doi = {10.1109/TNN.2003.810735},
	abstract = {Recent developments in the area of neural networks produced models capable of dealing with structured data. Here, we propose the first fully unsupervised model, namely an extension of traditional self-organizing maps (SOMs), for the processing of labeled directed acyclic graphs (DAGs). The extension is obtained by using the unfolding procedure adopted in recurrent and recursive neural networks, with the replicated neurons in the unfolded network comprising of a full SOM. This approach enables the discovery of similarities among objects including vectors consisting of numerical data. The capabilities of the model are analyzed in detail by utilizing a relatively large data set taken from an artificial benchmark problem involving visual patterns encoded as labeled DAGs. The experimental results demonstrate clearly that the proposed model is capable of exploiting both information conveyed in the labels attached to each node of the input DAGs and information encoded in the DAG topology.},
	language = {en},
	number = {3},
	urldate = {2018-12-10},
	journal = {IEEE Transactions on Neural Networks},
	author = {Hagenbuchner, M. and Sperduti, A. and {Ah Chung Tsoi}},
	month = may,
	year = {2003},
	pages = {491--505}
}


@inproceedings{Belghazi2018MutualIN,
  title={Mutual Information Neural Estimation},
  author={Mohamed Ishmael Belghazi and Aristide Baratin and Sai Rajeswar and Sherjil Ozair and Yoshua Bengio and R. Devon Hjelm and Aaron C. Courville},
  booktitle={ICML},
  year={2018}
}

@article{Bengio2013RepresentationLA,
  title={Representation Learning: A Review and New Perspectives},
  author={Yoshua Bengio and Aaron C. Courville and Pascal Vincent},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2013},
  volume={35},
  pages={1798-1828}
}

@article{Gunning2019XAIExplainableAI,
  title={XAI—Explainable artificial intelligence},
  author={David Gunning and Mark Stefik and Jaesik Choi and Timothy Miller and Simone Stumpf and Guang-Zhong Yang},
  journal={Science Robotics},
  year={2019},
  volume={4}
}

@article{2004kraskov,
   title={Estimating mutual information},
   volume={69},
   ISSN={1550-2376},
   url={http://dx.doi.org/10.1103/PhysRevE.69.066138},
   DOI={10.1103/physreve.69.066138},
   number={6},
   journal={Physical Review E},
   publisher={American Physical Society (APS)},
   author={Kraskov, Alexander and Stögbauer, Harald and Grassberger, Peter},
   year={2004},
   month={Jun}
}

@article{Cottrell1998TheoreticalAO,
  title={Theoretical aspects of the SOM algorithm},
  author={Marie Cottrell and Jean-Claude Fort and Gilles Pag{\`e}s},
  journal={Neurocomputing},
  year={1998},
  volume={21},
  pages={119-138}
}

@inproceedings{Cottrell2016TheoreticalAA,
  title={Theoretical and Applied Aspects of the Self-Organizing Maps},
  author={Marie Cottrell and Madalina Olteanu and Fabrice Rossi and Nathalie Villa-Vialaneix},
  booktitle={WSOM},
  year={2016}
}

@article{Timme2013SynergyRA,
  title={Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective},
  author={Nicholas M. Timme and Wesley Alford and Benjamin Flecker and John M. Beggs},
  journal={Journal of Computational Neuroscience},
  year={2013},
  volume={36},
  pages={119-140}
}

@article{Shannon1948AMT,
  title={A mathematical theory of communication},
  author={Claude E. Shannon},
  journal={Bell Syst. Tech. J.},
  year={1948},
  volume={27},
  pages={379-423}
}

@misc{hjelm2019learning,
      title={Learning deep representations by mutual information estimation and maximization}, 
      author={R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},
      year={2019},
      eprint={1808.06670},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC and Mac Kay, David JC},
  year={2003},
  publisher={Cambridge university press}
}

@article{Theil1961EconomicFA,
  title={Economic Forecasts And Policy Ed. 2nd},
  author={Henri Theil and J. S. Cramer and A. Russchen and H. Moerman},
  journal={Birchandra State Central Library,tripura},
  year={1961}
}

@inproceedings{Doquire2012ACO,
  title={A Comparison of Multivariate Mutual Information Estimators for Feature Selection},
  author={Gauthier Doquire and Michel Verleysen},
  booktitle={ICPRAM},
  year={2012}
}


@article{williams_nonnegative_2010,
	title = {Nonnegative {Decomposition} of {Multivariate} {Information}},
	url = {http://arxiv.org/abs/1004.2515},
	abstract = {Of the various attempts to generalize information theory to multiple variables, the most widely utilized, interaction information, suffers from the problem that it is sometimes negative. Here we reconsider from first principles the general structure of the information that a set of sources provides about a given variable. We begin with a new definition of redundancy as the minimum information that any source provides about each possible outcome of the variable, averaged over all possible outcomes. We then show how this measure of redundancy induces a lattice over sets of sources that clarifies the general structure of multivariate information. Finally, we use this redundancy lattice to propose a definition of partial information atoms that exhaustively decompose the Shannon information in a multivariate system in terms of the redundancy between synergies of subsets of the sources. Unlike interaction information, the atoms of our partial information decomposition are never negative and always support a clear interpretation as informational quantities. Our analysis also demonstrates how the negativity of interaction information can be explained by its confounding of redundancy and synergy.},
	language = {en},
	urldate = {2020-10-27},
	journal = {arXiv:1004.2515 [math-ph, physics:physics, q-bio]},
	author = {Williams, Paul L. and Beer, Randall D.},
	month = apr,
	year = {2010},
	note = {arXiv: 1004.2515},
	keywords = {94A15, Computer Science - Information Theory, Mathematical Physics, Physics - Biological Physics, Physics - Data Analysis, Statistics and Probability, Quantitative Biology - Neurons and Cognition, Quantitative Biology - Quantitative Methods},
	annote = {Comment: 14 pages, 9 figures},
	file = {Williams et Beer - 2010 - Nonnegative Decomposition of Multivariate Informat.pdf:/home/noemie/Zotero/storage/URZP7NXV/Williams et Beer - 2010 - Nonnegative Decomposition of Multivariate Informat.pdf:application/pdf},
}

@article{Bosking1997OrientationSA,
  title={Orientation Selectivity and the Arrangement of Horizontal Connections in Tree Shrew Striate Cortex},
  author={William H Bosking and Y. Zhang and Brett R. Schofield and David Fitzpatrick},
  journal={The Journal of Neuroscience},
  year={1997},
  volume={17},
  pages={2112 - 2127}
}

@article{Maaten2008VisualizingDU,
  title={Visualizing Data using t-SNE},
  author={Laurens van der Maaten and Geoffrey E. Hinton},
  journal={Journal of Machine Learning Research},
  year={2008},
  volume={9},
  pages={2579-2605}
}
